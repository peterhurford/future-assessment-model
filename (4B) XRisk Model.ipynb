{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1\n",
      "Loaded 2\n",
      "Using squiggle version 0.25\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import dill\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import squigglepy as sq\n",
    "import datetime as dt\n",
    "\n",
    "from squigglepy import bayes\n",
    "from squigglepy.numbers import K, M, B, T\n",
    "from copy import deepcopy\n",
    "from scipy import stats\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "print('Loaded 1')\n",
    "\n",
    "exec(open('utils.py').read())\n",
    "print('Loaded 2')\n",
    "\n",
    "print('Using squiggle version {}'.format(sq.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "VARS = {}\n",
    "\n",
    "# Global variables - probably don't want to change these but you could.\n",
    "VARS['RUNS'] = 10_000                                    # Number of runs to do (default 100*K)\n",
    "VARS['CURRENT_YEAR'] = 2023                              # What year to start the run on? (default: 2023)\n",
    "VARS['MAX_YEAR'] = 2023 + 100_000                        # What year to end the run on? (default: 2123)\n",
    "\n",
    "CURRENT_YEAR = VARS['CURRENT_YEAR']\n",
    "years = range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR'])\n",
    "print('Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAI Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TAI scenarios module\n"
     ]
    }
   ],
   "source": [
    "# Conditional on making agentic TAI, will it be aligned by default? Or will it otherwise fail to be power-seeking in a relevant way?\n",
    "VARS['p_tai_alignment_is_easy'] = 0.3\n",
    "VARS['p_tai_not_powerseeking'] = 0.4\n",
    "VARS['p_tai_aligned_by_default'] = VARS['p_tai_alignment_is_easy'] + (1 - VARS['p_tai_alignment_is_easy']) * VARS['p_tai_not_powerseeking']\\\n",
    "\n",
    "\n",
    "# Conditional on making agentic TAI that is not aligned by default, will we solve the alignment problem?\n",
    "# Varies by year, whether this is the first attempt, and whether there is a great power war\n",
    "def p_alignment_solved(year, first_attempt=True, verbose=False):\n",
    "    if first_attempt:\n",
    "        p = min(2.2 * (year/45), 0.8)\n",
    "    else:\n",
    "        p = min(2.8 * (year/45), 0.9)\n",
    "    if verbose == 2:\n",
    "        print('* alignment solve diagnostic - war: {} year: {} first attempt: {} -> p {})'.format(war, year, first_attempt, p))\n",
    "    return p\n",
    "VARS['p_alignment_solved'] = p_alignment_solved\n",
    "# TODO: Convert to logistic curves\n",
    "\n",
    "\n",
    "# Conditional on making agentic TAI that is not aligned by default, will we successfully coordinate to deploy safe AI?\n",
    "# Varies by year, whether this is the first attempt, and whether there is a great power war\n",
    "def p_alignment_deployment_safety_and_coordination(year, war, variables, first_attempt=True, verbose=False):\n",
    "    if year <= 2030 - variables['CURRENT_YEAR'] and not war:\n",
    "        p = 0.8\n",
    "    elif war:\n",
    "        p = 0.4\n",
    "    else:\n",
    "        p = 0.6\n",
    "    if first_attempt is False:\n",
    "        p += 0.1\n",
    "    p = min(p, 0.95)\n",
    "    p = max(p, 0.05)\n",
    "    if verbose == 2:\n",
    "        print('* alignment coordination diagnostic - war: {} year: {} first attempt: {} -> p {})'.format(war, year, first_attempt, p))\n",
    "    return p\n",
    "VARS['p_alignment_deploy_coordination'] = p_alignment_deployment_safety_and_coordination\n",
    "\n",
    "# Conditional on solving the alignment problem, what is the chance we also solve the subtle misalignment problem?\n",
    "VARS['p_subtle_alignment_solved'] = 0.85\n",
    "\n",
    "# Conditional on alignment by default, what is the chance we also solve the subtle misalignment problem?\n",
    "VARS['p_subtle_alignment_solved_if_aligned_by_default'] = 0.4\n",
    "\n",
    "# Conditional on having aligned AI, will we know it is aligned and therefore want to deploy it?\n",
    "VARS['p_know_aligned_ai_is_aligned'] = 0.6\n",
    "\n",
    "# Conditional on having misaligned AI, will we know it is misaligned and therefore not want to deploy it?\n",
    "VARS['p_know_misaligned_ai_is_misaligned'] = 0.7\n",
    "\n",
    "# Conditional on having agentic TAI, will it be intentionally misused to create a singleton?\n",
    "def p_tai_intentional_misuse(war):\n",
    "    return 0.2 if war else 0.02\n",
    "VARS['p_tai_intentional_misuse'] = p_tai_intentional_misuse\n",
    "\n",
    "# If TAI is fully misaligned what is the chance we can successfully detect and avert this?\n",
    "VARS['p_full_tai_misalignment_averted'] = 0.2\n",
    "\n",
    "# If TAI is fully misaligned but successfully averted, what is the probability there will be a catasrophe (10%+ death)?\n",
    "VARS['p_tai_misalignment_averting_is_catastrophic'] = 0.4\n",
    "\n",
    "# If TAI is fully misaligned and we successfully avert it, what is the chance we give up on TAI?\n",
    "VARS['p_full_tai_misalignment_averted_means_abandoned_tai'] = 0.7\n",
    "\n",
    "# If TAI is fully misaligned, what is the chance it results in extinction versus a singleton?\n",
    "VARS['p_tai_xrisk_is_extinction'] = 0.1\n",
    "\n",
    "# If there is a fully misaligned TAI singleton, what is the chance it results in a non-extinction catastrophe (10%+ death)?\n",
    "VARS['p_tai_singleton_is_catastrophic'] = 0.8\n",
    "\n",
    "# If ther is an intentional attempt to misuse TAI to create a singleton, what is the chance it causes extinction instead?\n",
    "VARS['p_intentional_tai_singleton_creates_extinction'] = 0.05\n",
    "\n",
    "\n",
    "exec(open('modules/tai_risk.py').read())\n",
    "print('Loaded TAI scenarios module')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MixtureDistribution' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/IPython/lib/pretty.py:393\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _get_mro(obj_class):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_pprinters:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# printer registered in self.type_pprinters\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_pprinters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;66;03m# deferred printer\u001b[39;00m\n\u001b[1;32m    396\u001b[0m         printer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_deferred_types(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/IPython/lib/pretty.py:692\u001b[0m, in \u001b[0;36m_dict_pprinter_factory.<locals>.inner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    690\u001b[0m     p\u001b[38;5;241m.\u001b[39mpretty(key)\n\u001b[1;32m    691\u001b[0m     p\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 692\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m p\u001b[38;5;241m.\u001b[39mend_group(step, end)\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/IPython/lib/pretty.py:393\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _get_mro(obj_class):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_pprinters:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# printer registered in self.type_pprinters\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_pprinters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;66;03m# deferred printer\u001b[39;00m\n\u001b[1;32m    396\u001b[0m         printer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_deferred_types(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/IPython/lib/pretty.py:692\u001b[0m, in \u001b[0;36m_dict_pprinter_factory.<locals>.inner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    690\u001b[0m     p\u001b[38;5;241m.\u001b[39mpretty(key)\n\u001b[1;32m    691\u001b[0m     p\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 692\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m p\u001b[38;5;241m.\u001b[39mend_group(step, end)\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/squigglepy/distributions.py:42\u001b[0m, in \u001b[0;36mBaseDistribution.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/squigglepy/distributions.py:1400\u001b[0m, in \u001b[0;36mMixtureDistribution.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1400\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m)\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdists)):\n\u001b[1;32m   1402\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m weight on \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdists[i])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MixtureDistribution' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "# PROBABILITY OF A NONSCALING DELAY\n",
    "\n",
    "# set to None to have no delay\n",
    "# Otherwise specified in a dictionary\n",
    "# {'delay': {'prob': <array of probabilities by year>, 'length': <distribution to sample from to get length of delay>}}\n",
    "\n",
    "# Cache defined in \"(3B) Nonscaling Delay Curve\"\n",
    "\n",
    "with open('caches/nonscaling_delays.dill', 'rb') as f:\n",
    "    delay = dill.load(f)\n",
    "\n",
    "VARS['delay'] = delay\n",
    "print('Loaded')\n",
    "delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "# Defined in \"(1) Anchors\"\n",
    "#\n",
    "# set `threat_models = None` to not use this feature\n",
    "#\n",
    "with open('caches/threat_models.dill', 'rb') as f:\n",
    "    threat_models = dill.load(f)\n",
    "\n",
    "VARS['threat_model'] = threat_models\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuclear Scenarios Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nuclear scenarios module\n"
     ]
    }
   ],
   "source": [
    "# Annual chance Russia uses a nuke\n",
    "def p_russia_uses_nuke(peace, year, variables):\n",
    "    peace = 10 if peace else 1\n",
    "    year = year + variables['CURRENT_YEAR']\n",
    "    if year == 2023:\n",
    "        return 0.03\n",
    "    else:\n",
    "        return 0.001 / peace\n",
    "VARS['p_russia_uses_nuke'] = p_russia_uses_nuke\n",
    "\n",
    "\n",
    "# Annual chance North Korea uses a nuke\n",
    "VARS['p_nk_uses_nuke'] = 0.001\n",
    "\n",
    "\n",
    "# Annual chance China uses a nuke\n",
    "def p_china_uses_nuke(peace, year, variables):\n",
    "    return VARS['p_china_invades_taiwan'](peace, year, variables) * 0.01\n",
    "VARS['p_china_uses_nuke'] = p_china_uses_nuke\n",
    "\n",
    "    \n",
    "# Annual chance another country uses a nuke\n",
    "def p_other_uses_nuke(peace):\n",
    "    peace = 10 if peace else 1\n",
    "    return 0.0002 / peace\n",
    "VARS['p_other_uses_nuke'] = p_other_uses_nuke\n",
    "\n",
    "\n",
    "# What is the chance in a given year there will be a \"nuclear accident\"?\n",
    "def p_nuclear_accident(war, year):\n",
    "    p = 0.05 if war else 0.02\n",
    "    p = p * (0.998 ** year)\n",
    "    return p\n",
    "VARS['p_nuclear_accident'] = p_nuclear_accident\n",
    "\n",
    "\n",
    "# Conditional on a nuclear accident, what is the chance it escalates into an \"exchange\"?\n",
    "def p_nuclear_accident_becomes_exchange(war):\n",
    "    return 0.2 if war else 0.05\n",
    "VARS['p_nuclear_accident_becomes_exchange'] = p_nuclear_accident_becomes_exchange\n",
    "\n",
    "\n",
    "# Conditional on a nuclear exchange, what is the chance it escalates into a catastrophe (10%+ dead)?\n",
    "def p_catastrophe_from_nuclear_exchange(war):\n",
    "    p_exchange_becomes_all_out_war = 0.6 if war else 0.3\n",
    "    p_nuclear_winter_happens = 0.3\n",
    "    alternative_foods_or_other_save = 0.05\n",
    "    return (p_exchange_becomes_all_out_war *\n",
    "            (p_nuclear_winter_happens + (1 - p_nuclear_winter_happens) * 0.1) *\n",
    "            (1 - alternative_foods_or_other_save))\n",
    "VARS['p_catastrophe_from_nuclear_exchange'] = p_catastrophe_from_nuclear_exchange\n",
    "\n",
    "    \n",
    "# Conditional on a nuclear exchange catastrophe, what is the chance it becomes an xrisk?\n",
    "VARS['p_xrisk_from_nuclear_catastrophe'] = 0.05 # https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\n",
    "\n",
    "\n",
    "# Conditional on a great power war, what is the chance it goes intentionally nuclear in any given year?\n",
    "def p_nuclear_exchange_given_war(first_year_of_war):\n",
    "    return 0.1 if first_year_of_war else 0.02\n",
    "VARS['p_nuclear_exchange_given_war'] = p_nuclear_exchange_given_war\n",
    "\n",
    "\n",
    "exec(open('modules/nuclear.py').read())\n",
    "print('Loaded nuclear scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Power War Scenarios Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded great power war scenarios module\n"
     ]
    }
   ],
   "source": [
    "# Annual chance that, conditional on Russia not nuking outside of a great power war, there will be a US-Russia great power war\n",
    "def p_great_power_war_us_russia_without_nuke_first(peace, year, variables):\n",
    "    peace = 20 if peace else 1\n",
    "    year = year + variables['CURRENT_YEAR']\n",
    "    if year <= 2025:\n",
    "        return (0.01 / 2) / peace\n",
    "    else:\n",
    "        return 0.002 / peace\n",
    "VARS['p_great_power_war_us_russia_without_nuke_first'] = p_great_power_war_us_russia_without_nuke_first\n",
    "\n",
    "\n",
    "# Annual chance China invades Taiwan\n",
    "def p_china_invades_taiwan(peace, year, variables):\n",
    "    peace = 10 if peace else 1\n",
    "    year = year + variables['CURRENT_YEAR']\n",
    "    if year == 2022:\n",
    "        return 0\n",
    "    elif year == 2023:\n",
    "        return 0.01\n",
    "    elif year == 2024 or year == 2025:\n",
    "        return 0.0255   # makes cumulative probability by EOY 2025 = 0.06\n",
    "                        # solve 0.01 + (1-0.01)*X + (1-0.01)(1-X)*X = 0.06, 0>X<1\n",
    "    elif year < 2030:\n",
    "        return 0.0493 # makes cumulative probability by EOY 2029 = 0.27\n",
    "                      # solve 0.06 + (1-0.06)*X + (1-0.06)(1-X)*X + (1-0.06)(1-X)^2*X + (1-0.06)(1-X)^3*X + (1-0.06)(1-X)^4*X = 0.27\n",
    "    elif year < 2035:\n",
    "        return 0.0352 # makes cumulative probability by EOY 2034 = 0.39\n",
    "                      # 0.27 + (1-0.27)*X + (1-0.27)(1-X)*X + (1-0.27)(1-X)^2*X + (1-0.27)(1-X)^3*X + (1-0.27)(1-X)^4*X = 0.39\n",
    "    else:\n",
    "        return 0.005 / peace\n",
    "VARS['p_china_invades_taiwan'] = p_china_invades_taiwan\n",
    "\n",
    "    \n",
    "# Annual chance that there is a great power war between the US and China\n",
    "def p_great_power_war_us_china(peace, year, variables):\n",
    "    p_invade_taiwan = variables['p_china_invades_taiwan'](peace, year, variables)\n",
    "    p_us_responds = 0.7\n",
    "    return p_invade_taiwan * p_us_responds\n",
    "VARS['p_great_power_war_us_china'] = p_great_power_war_us_china\n",
    "\n",
    "    \n",
    "# Annual chance that there is some great power war other than US<>Russia and US<>China\n",
    "def p_great_power_war_other(peace, year, variables):\n",
    "    peace = 5 if peace else 1\n",
    "    year = year + variables['CURRENT_YEAR']\n",
    "    if year > 2040:\n",
    "        return 0.003 / peace\n",
    "    else:\n",
    "        return 0.0005 / peace\n",
    "VARS['p_great_power_war_other'] = p_great_power_war_other\n",
    "\n",
    "\n",
    "# Conditional on a great power war starting, how long will it last?\n",
    "VARS['war_length'] = sq.lognorm(2, 50) # 90% CI\n",
    "\n",
    "# After a war ends, how long will there be a peace?\n",
    "VARS['peace_length'] = sq.lognorm(10, 100)\n",
    "\n",
    "\n",
    "exec(open('modules/great_power_war.py').read())\n",
    "print('Loaded great power war scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio scenarios module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bio scenarios module\n"
     ]
    }
   ],
   "source": [
    "# Conditional on a great power war, what is the annual chance it intentionally results in a bioweapon?\n",
    "VARS['p_biowar_given_war'] = 1/800\n",
    "\n",
    "# What is the annual chance of a non-state actor creating an intentional biorisk that causes 1%+ death?\n",
    "VARS['p_nonstate_bio'] = 1/600\n",
    "\n",
    "# What is the chance that if 1%+ die from natural bio, 10%+ will die from natural bio?\n",
    "VARS['p_natural_bio_is_catastrophe'] = 1 / (10 ** 0.5) # https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028\n",
    "\n",
    "# What is the chance that if 1%+ die from engineered bio, 10%+ will die from engineered bio?\n",
    "VARS['p_engineered_bio_is_catastrophe'] = 1 / (10 ** 0.5) # https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028\n",
    "\n",
    "VARS['p_covid_spanish_flu_like_becomes_1pct_death'] = 1 / (10 ** 0.5) # https://www.economist.com/graphic-detail/coronavirus-excess-deaths-estimates suggests COVID killed 0.2%... https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028 suggests 1% is 2x less likely than 0.2%\n",
    "VARS['p_covid_lab_leak'] = 0.1\n",
    "VARS['p_extinction_given_90_pct_death'] = 0.03 # per Luisa https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\n",
    "VARS['p_accidental_catastrophe_causes_90_pct_death'] = (1 / (10 ** 0.5)) ** 2 # https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028\n",
    "VARS['p_intentional_catastrophe_causes_90_pct_death'] = (1 / (10 ** 0.5)) ** 2 # https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028\n",
    "\n",
    "# If a lab leak occurs, how likely is it that the leaked pandemic will be engineered vs. natural?\n",
    "VARS['ratio_engineered_vs_natural_lab_leak'] = 0.8\n",
    "\n",
    "# What is the chance of an natural biorisk/pandemic causing 1%+ population death?\n",
    "def p_natural_bio(year, variables):\n",
    "    base_rate_from_covid_and_spanish_flu = 1/250\n",
    "    increase_from_globalization = 1.1\n",
    "    decreate_in_rate_per_year_from_improvements = 0.99 ** year\n",
    "    return ((base_rate_from_covid_and_spanish_flu * 0.5 +\n",
    "             base_rate_from_covid_and_spanish_flu * (1 - variables['p_covid_lab_leak']) * 0.5) *\n",
    "            increase_from_globalization *\n",
    "            variables['p_covid_spanish_flu_like_becomes_1pct_death'] *\n",
    "            decreate_in_rate_per_year_from_improvements)\n",
    "VARS['p_natural_bio'] = p_natural_bio\n",
    "    \n",
    "    \n",
    "# What is the chance of an accidental biorisk (e.g., lab leak) causing 1%+ population death?\n",
    "def p_accidental_bio(war, variables):\n",
    "    base_rate_from_covid = 0.01 * variables['p_covid_lab_leak']\n",
    "    increase_factor_due_to_increasing_labs = 1.3\n",
    "    increase_factor_due_to_great_power_war = 2\n",
    "    p = (base_rate_from_covid *\n",
    "         variables['p_covid_spanish_flu_like_becomes_1pct_death'] *\n",
    "         increase_factor_due_to_increasing_labs)\n",
    "    return p * increase_factor_due_to_great_power_war if war else p\n",
    "VARS['p_accidental_bio'] = p_accidental_bio    \n",
    "\n",
    "\n",
    "# Conditional on a accidental biorisk (1% death), what is the chance it becomes a xrisk?\n",
    "def p_xrisk_from_accidental_bio_given_catastrophe(year, variables):\n",
    "    return variables['p_accidental_catastrophe_causes_90_pct_death'] * variables['p_extinction_given_90_pct_death']\n",
    "VARS['p_xrisk_from_accidental_bio_given_catastrophe'] = p_xrisk_from_accidental_bio_given_catastrophe\n",
    "\n",
    "\n",
    "# Conditional on a bioweapon, what is the chance it becomes a xrisk?\n",
    "def p_xrisk_from_engineered_bio_given_catastrophe(year, variables):\n",
    "    return variables['p_intentional_catastrophe_causes_90_pct_death'] * variables['p_extinction_given_90_pct_death']\n",
    "VARS['p_xrisk_from_engineered_bio_given_catastrophe'] = p_xrisk_from_engineered_bio_given_catastrophe\n",
    "\n",
    "\n",
    "exec(open('modules/bio.py').read())\n",
    "print('Loaded bio scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nanotech scenarios module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nano scenarios module\n"
     ]
    }
   ],
   "source": [
    "# What is the chance in a given year that nanotech will be developed?\n",
    "def p_nanotech_possible(year):\n",
    "    if year < 200:\n",
    "        return 0.0001 / (0.956 ** year)\n",
    "    else:\n",
    "        return 1 # TODO: This is dumb\n",
    "VARS['p_nanotech_possible'] = p_nanotech_possible\n",
    "\n",
    "\n",
    "# Conditional on developing nanotech, what is the chance nanotech results in an xrisk?\n",
    "VARS['p_nanotech_is_xrisk'] = 0.1 * 0.05\n",
    "\n",
    "\n",
    "exec(open('modules/nano.py').read())\n",
    "print('Loaded nano scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervolcano scenarios module (all other natural risks <0.01%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded supervolcano module\n"
     ]
    }
   ],
   "source": [
    "VARS['p_supervolcano_catastrophe'] = 1 / (500*K)  # https://www.openphilanthropy.org/research/large-volcanic-eruptions/ VEI >= 9 (geometric mean of 30K and 30M)\n",
    "\n",
    "VARS['p_supervolcano_extinction_given_catastrophe'] = 0.05\n",
    "\n",
    "\n",
    "exec(open('modules/supervolcano.py').read())\n",
    "print('Loaded supervolcano module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown unknown scenarios module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded unknown unknown scenarios module\n"
     ]
    }
   ],
   "source": [
    "# What is the chance in any given year that an unknown unknown xrisk occurs?\n",
    "def p_unknown_unknown_xrisk(year):\n",
    "    return 1 / (100*K) / (0.99 ** min(230, year)) # TODO: This is dumb\n",
    "VARS['p_unknown_unknown_xrisk'] = p_unknown_unknown_xrisk\n",
    "\n",
    "\n",
    "exec(open('modules/unknown_unknown.py').read())\n",
    "print('Loaded unknown unknown scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double dip catastrophe module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded double dip catastrophe module\n"
     ]
    }
   ],
   "source": [
    "VARS['p_extinction_from_double_catastrophe'] = 0.1\n",
    "\n",
    "VARS['extinction_from_double_catastrophe_range'] = 10\n",
    "\n",
    "exec(open('modules/double_dip_catastrophe.py').read())\n",
    "print('Loaded double dip catastrophe module')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Timeline variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TAI timelines module\n",
      "-\n",
      "Loading from cache file (`caches/tai_years.sqcache`)...\n",
      "...Loaded\n",
      "Caching in-memory...\n",
      "...Cached!\n",
      "...Reducing\n",
      "...Reduced!\n",
      "...All done!\n",
      "-\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "VARS['if_catastrophe_delay_tai_arrival_by_years'] = sq.lognorm(3, 15)\n",
    "\n",
    "VARS['if_us_china_war_delay_tai_arrival_by_years'] = sq.lognorm(3, 15)\n",
    "\n",
    "exec(open('modules/tai_timelines.py').read())\n",
    "print('Loaded TAI timelines module')\n",
    "\n",
    "print('-')\n",
    "VARS['tai_years'] = bayes.bayesnet(load_cache_file='caches/tai_years', verbose=True)\n",
    "\n",
    "print('-')\n",
    "print('Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded time of perils module\n"
     ]
    }
   ],
   "source": [
    "VARS['tai_ends_time_of_perils'] = 0.2\n",
    "\n",
    "VARS['extinction_is_morally_good_actually'] = 0\n",
    "\n",
    "VARS['misaligned_tai_takeover_is_still_morally_fine'] = 0.1\n",
    "\n",
    "exec(open('modules/time_of_perils.py').read())\n",
    "print('Loaded time of perils module')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached variables!\n"
     ]
    }
   ],
   "source": [
    "with open('caches/variables.dill', 'wb') as f:\n",
    "    dill.dump(VARS, f)\n",
    "print('cached variables!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "## RUN 1 ##\n",
      "############\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type NoneType doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m## RUN \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ##\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m############\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mdefine_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVARS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m25\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m############\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<string>:46\u001b[0m, in \u001b[0;36mdefine_event\u001b[0;34m(variables, verbosity)\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/squigglepy/samplers.py:704\u001b[0m, in \u001b[0;36msample\u001b[0;34m(dist, n, lclip, rclip, memcache, reload_cache, dump_cache_file, load_cache_file, cache_file_primary, verbose, cores, _multicore_tqdm_n, _multicore_tqdm_cores)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Handle loading from cache\u001b[39;00m\n\u001b[1;32m    703\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 704\u001b[0m has_in_mem_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m _squigglepy_internal_sample_caches\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_cache_file:\n\u001b[1;32m    706\u001b[0m     cache_path \u001b[38;5;241m=\u001b[39m load_cache_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.sqcache.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/squigglepy/distributions.py:173\u001b[0m, in \u001b[0;36mComplexDistribution.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfix:\n\u001b[1;32m    172\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str,\n\u001b[0;32m--> 173\u001b[0m                                          \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfix:\n\u001b[1;32m    175\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    176\u001b[0m                                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str,\n\u001b[1;32m    177\u001b[0m                                            \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/squigglepy/distributions.py:175\u001b[0m, in \u001b[0;36mComplexDistribution.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str,\n\u001b[1;32m    173\u001b[0m                                          \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfix:\n\u001b[0;32m--> 175\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    176\u001b[0m                                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str,\n\u001b[1;32m    177\u001b[0m                                            \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfix:\n\u001b[1;32m    179\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/squigglepy/distributions.py:175\u001b[0m, in \u001b[0;36mComplexDistribution.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str,\n\u001b[1;32m    173\u001b[0m                                          \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfix:\n\u001b[0;32m--> 175\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    176\u001b[0m                                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str,\n\u001b[1;32m    177\u001b[0m                                            \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfix:\n\u001b[1;32m    179\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: ComplexDistribution.__str__ at line 175 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/squigglepy/distributions.py:175\u001b[0m, in \u001b[0;36mComplexDistribution.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str,\n\u001b[1;32m    173\u001b[0m                                          \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfix:\n\u001b[0;32m--> 175\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    176\u001b[0m                                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_str,\n\u001b[1;32m    177\u001b[0m                                            \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfix:\n\u001b[1;32m    179\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/dev/lib/python3.11/site-packages/squigglepy/distributions.py:752\u001b[0m, in \u001b[0;36mLognormalDistribution.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    751\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Distribution> \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(mean=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, sd=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype,\n\u001b[0;32m--> 752\u001b[0m                                                     \u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    753\u001b[0m                                                     \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msd, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlclip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, lclip=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlclip)\n",
      "\u001b[0;31mTypeError\u001b[0m: type NoneType doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "exec(open('modules/define_event.py').read())\n",
    "\n",
    "for i in range(5):\n",
    "    print('############')\n",
    "    print('## RUN {} ##'.format(i + 1))\n",
    "    print('############')\n",
    "    define_event(VARS, verbosity=2)\n",
    "\n",
    "for i in range(5, 25):\n",
    "    print('############')\n",
    "    print('## RUN {} ##'.format(i + 1))\n",
    "    print('############')\n",
    "    define_event(VARS, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "define_event_lambda = lambda: define_event(VARS, verbosity=0)\n",
    "collectors = bayes.bayesnet(define_event_lambda,\n",
    "                            load_cache_file='caches/future_assessment_model_cache',\n",
    "                            dump_cache_file='caches/future_assessment_model_cache',\n",
    "                            reload_cache=False,#True,\n",
    "                            raw=True,\n",
    "                            verbose=True,\n",
    "                            cores=5,\n",
    "                            n=VARS['RUNS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('4E. When TAI?')\n",
    "print('-')\n",
    "\n",
    "yrs = bayes.bayesnet(define_event_lambda,\n",
    "                     find=lambda e: e['tai_year'],\n",
    "                     raw=True,\n",
    "                     n=VARS['RUNS'])\n",
    "yrs = [VARS['MAX_YEAR'] + 1 if y is None else y for y in yrs]\n",
    "print_tai_arrival_stats(yrs, VARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if years[-1] > 2200:\n",
    "    years = list(range(VARS['CURRENT_YEAR'], 2200))\n",
    "\n",
    "alignment_p = np.array([p_alignment_solved(year=y - VARS['CURRENT_YEAR'], first_attempt=True) for y in years])\n",
    "alignment_p2 = np.array([p_alignment_solved(year=y - VARS['CURRENT_YEAR'], first_attempt=False) for y in years])\n",
    "plt.plot(years, alignment_p, label='first attempt')\n",
    "plt.plot(years, alignment_p2, label='2+ attempt')\n",
    "plt.legend()\n",
    "plt.ylabel('chance of solving alignment if TAI in year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in list(years[:17]) + list(years[17::10]):\n",
    "    str_ = 'Year: {} - chance of solving TAI alignment {}% (2nd attempt {}%)'\n",
    "    print(str_.format(y,\n",
    "                      round(alignment_p[y - VARS['CURRENT_YEAR']] * 100, 0),\n",
    "                      round(alignment_p2[y - VARS['CURRENT_YEAR']] * 100, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordination_p = np.array([p_alignment_deployment_safety_and_coordination(year=y - VARS['CURRENT_YEAR'], war=False, variables=VARS, first_attempt=True) for y in years])\n",
    "coordination_p2 = np.array([p_alignment_deployment_safety_and_coordination(year=y - VARS['CURRENT_YEAR'], war=False, variables=VARS, first_attempt=False) for y in years])\n",
    "coordination_p_war = np.array([p_alignment_deployment_safety_and_coordination(year=y - VARS['CURRENT_YEAR'], war=True, variables=VARS, first_attempt=True) for y in years])\n",
    "coordination_p2_war = np.array([p_alignment_deployment_safety_and_coordination(year=y - VARS['CURRENT_YEAR'], war=True, variables=VARS, first_attempt=False) for y in years])\n",
    "plt.plot(years, coordination_p, label='no war, first attempt')\n",
    "plt.plot(years, coordination_p2, label='no war, 2+ attempt')\n",
    "plt.plot(years, coordination_p_war, label='war, first attempt')\n",
    "plt.plot(years, coordination_p2_war, label='war, 2+ attempt')\n",
    "plt.legend()\n",
    "plt.ylabel('chance of coordinating deployment if TAI in year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in list(years[:17]) + list(years[17::10]):\n",
    "    str_ = 'Year: {} - chance of coordinating deployment no war -- {}% (2nd attempt {}%); war -- {}% (2nd attempt {}%)'\n",
    "    print(str_.format(y,\n",
    "                      round(coordination_p[y - VARS['CURRENT_YEAR']] * 100, 0),\n",
    "                      round(coordination_p2[y - VARS['CURRENT_YEAR']] * 100, 0),\n",
    "                      round(coordination_p_war[y - VARS['CURRENT_YEAR']] * 100, 0),\n",
    "                      round(coordination_p2_war[y - VARS['CURRENT_YEAR']] * 100, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AI X-Risk BY EOY year')\n",
    "\n",
    "def find(y_c, category):\n",
    "    return bayes.bayesnet(define_event_lambda,\n",
    "                          find=lambda e: e['category'] == category and e['final_year'] <= y_c,\n",
    "                          n=VARS['RUNS'])\n",
    "\n",
    "target_years = [2023, 2024, 2025, 2026, 2027, 2028, 2030, 2035, 2040, 2050, 2060, 2070,\n",
    "                2100, 2200, 2300, 2400, 2500, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10_000]\n",
    "for y_c in target_years:\n",
    "    extinction = find(y_c, 'xrisk_full_unaligned_tai_extinction')\n",
    "    singleton = find(y_c, 'xrisk_full_unaligned_tai_singleton')\n",
    "    subtle_misalignment = find(y_c, 'xrisk_subtly_unaligned_tai')\n",
    "    misuse = find(y_c, 'xrisk_tai_misuse') + find(y_c, 'xrisk_tai_misuse_extinction')\n",
    "    out = '{} - {}% (Extinction: {}%, Bad TAI singleton: {}%, Subtly misaligned singleton: {}%, Misuse singleton: {}%)'\n",
    "    print(out.format(y_c,\n",
    "                     round((extinction + singleton + subtle_misalignment + misuse) * 100, 1),\n",
    "                     round(extinction * 100, 1),\n",
    "                     round(singleton * 100, 1),\n",
    "                     round(subtle_misalignment * 100, 1),\n",
    "                     round(misuse * 100, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Chance of successfully aligning TAI by EOY year')\n",
    "for y_c in target_years:\n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: 'aligned_tai' in e['category'] and 'unaligned' not in e['category'] and e['final_year'] <= y_c,\n",
    "                       n=VARS['RUNS'])\n",
    "    r1 = bayes.bayesnet(define_event_lambda,\n",
    "                        find=lambda e: e['category'] == 'aligned_tai_ends_time_of_perils' and e['final_year'] <= y_c,\n",
    "                        n=VARS['RUNS'])\n",
    "    r2 = bayes.bayesnet(define_event_lambda,\n",
    "                        find=lambda e: e['category'] == 'aligned_tai_does_not_end_time_of_perils' and e['final_year'] <= y_c,\n",
    "                        n=VARS['RUNS'])\n",
    "    print('{} - {}% ({}% ends time of perils, {}% does not)'.format(y_c,\n",
    "                                                                    round(r * 100, 2),\n",
    "                                                                    round(r1 * 100, 2),\n",
    "                                                                    round(r2 * 100, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cumulative Total X-Risk (including non-extinction x-risks and \"ok but not great\" x-risks)')\n",
    "\n",
    "def find_xrisk(y_c, category):\n",
    "    return bayes.bayesnet(define_event_lambda,\n",
    "                          find=lambda e: 'xrisk' in e['category'] and category in e['category'] and e['final_year'] <= y_c,\n",
    "                          n=VARS['RUNS'])\n",
    "\n",
    "for y_c in target_years:\n",
    "    ai = find_xrisk(y_c, 'tai')\n",
    "    nukes = find_xrisk(y_c, 'nukes')\n",
    "    unknown = find_xrisk(y_c, 'unknown')\n",
    "    nano = find_xrisk(y_c, 'nanotech')\n",
    "    natural = find_xrisk(y_c, 'supervolcano')\n",
    "    bio = find_xrisk(y_c, 'bio')\n",
    "\n",
    "    out = '{} - {}% (AI: {}%, Nukes: {}%, Bio: {}%, Nano: {}%, Natural: {}%, Unknown unknown: {}%)'\n",
    "    print(out.format(y_c,\n",
    "                     round((ai + nukes + bio + nano + unknown) * 100, 2),\n",
    "                     round(ai * 100, 2),\n",
    "                     round(nukes * 100, 3),\n",
    "                     round(bio * 100, 3),\n",
    "                     round(nano * 100, 3),\n",
    "                     round(natural * 100, 3),\n",
    "                     round(unknown * 100, 3)))\n",
    "\n",
    "print('-')\n",
    "print('AI risk is {}% of total risk'.format(round(ai / (ai + nukes + bio + nano + unknown) * 100, 2)))\n",
    "print('AI risk is {}% of known risk'.format(round(ai / (ai + nukes + bio + nano) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cumulative Total Extinction Risk')\n",
    "\n",
    "for y_c in target_years:\n",
    "    ai = find(y_c, 'xrisk_full_unaligned_tai_extinction') + find(y_c, 'xrisk_tai_misuse_extinction')\n",
    "    nukes_war = find(y_c, 'xrisk_nukes_war')\n",
    "    nukes_accident = find(y_c, 'xrisk_nukes_accident')\n",
    "    unknown = find(y_c, 'xrisk_unknown_unknown')\n",
    "    bio_war = find(y_c, 'xrisk_bio_war')\n",
    "    bio_accident = find(y_c, 'xrisk_bio_accident')\n",
    "    bio_nonstate = find(y_c, 'xrisk_bio_nonstate')\n",
    "    nanotech = find(y_c, 'xrisk_nanotech')\n",
    "    supervolcano = find(y_c, 'xrisk_supervolcano')\n",
    "    \n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: e['category'] in extinctions and e['final_year'] <= y_c,\n",
    "                       n=VARS['RUNS'])\n",
    "    \n",
    "    out = '{} - {}% (AI: {}%, Nukes: {}% (War: {}% Accident: {}%), Bio: {}% (War: {}%, Accident: {}%, Nonstate: {}%), Nano: {}%, Natural: {}%, Other: {}%)'\n",
    "    print(out.format(y_c,\n",
    "                     round(r * 100, 2),\n",
    "                     round(ai * 100, 2),\n",
    "                     round((nukes_war + nukes_accident) * 100, 3),\n",
    "                     round(nukes_war * 100, 3),\n",
    "                     round(nukes_accident * 100, 3),\n",
    "                     round((bio_war + bio_accident + bio_nonstate) * 100, 3),\n",
    "                     round(bio_war * 100, 3),\n",
    "                     round(bio_accident * 100, 3),\n",
    "                     round(bio_nonstate * 100, 3),\n",
    "                     round(nanotech * 100, 3),\n",
    "                     round(supervolcano * 100, 3),\n",
    "                     round(unknown * 100, 3)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.metaculus.com/questions/1493/global-population-decline-10-by-2100/\n",
    "print('Cumulative Total Catastrophe Risk (defined as 10%+ death)')\n",
    "\n",
    "for y_c in target_years:\n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: (len(e['catastrophe']) > 0 and e['catastrophe'][0]['year'] <= y_c) or (e['category'] in extinctions and e['final_year'] <= y_c),\n",
    "                       n=VARS['RUNS'])\n",
    "    print('{} - {}%'.format(y_c, round(r * 100, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cumulative Total *Actively Bad* Future X-Risk (including non-extinction risks but excluding subtle AI misalignment)')\n",
    "\n",
    "for y_c in target_years:\n",
    "    ai = find_xrisk(y_c, 'tai')\n",
    "    ai_subtle_misalignment = find_xrisk(y_c, 'xrisk_subtly_unaligned_tai')\n",
    "    nukes = find_xrisk(y_c, 'nukes')\n",
    "    unknown = find_xrisk(y_c, 'unknown')\n",
    "    nano = find_xrisk(y_c, 'nanotech')\n",
    "    natural = find_xrisk(y_c, 'supervolcano')\n",
    "    bio = find_xrisk(y_c, 'bio')\n",
    "\n",
    "    out = '{} - {}% (AI: {}%, Nukes: {}%, Bio: {}%, Nano: {}%, Natural: {}%, Other: {}%)'\n",
    "    print(out.format(y_c,\n",
    "                     round((ai - ai_subtle_misalignment + nukes + bio + nano + unknown) * 100, 2),\n",
    "                     round((ai - ai_subtle_misalignment) * 100, 2),\n",
    "                     round(nukes * 100, 3),\n",
    "                     round(bio * 100, 3),\n",
    "                     round(nano * 100, 3),\n",
    "                     round(natural * 100, 3),\n",
    "                     round(unknown * 100, 3)))\n",
    "\n",
    "print('-')\n",
    "print('AI risk is {}% of total risk'.format(round((ai - ai_subtle_misalignment) / ((ai - ai_subtle_misalignment) + nukes + bio + nano + unknown) * 100, 2)))\n",
    "print('AI risk is {}% of known risk'.format(round((ai - ai_subtle_misalignment) / ((ai - ai_subtle_misalignment) + nukes + bio + nano) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_states(collectors, y_c):\n",
    "    states = [c['category'] if c['final_year'] <= y_c else 'boring' for c in collectors]\n",
    "    c = Counter(states)\n",
    "    c = dict([(k, round(v / VARS['RUNS'] * 100, 3)) for k, v in c.items()])\n",
    "    for k in c.keys():\n",
    "        if k not in FUTURES:\n",
    "            raise ValueError('Future {} not in `FUTURES`'.format(k))\n",
    "    for state in FUTURES:\n",
    "        if not c.get(state):\n",
    "            c[state] = 0.0\n",
    "    return sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "print('Detail on World State At Year')\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c)) \n",
    "    pprint(print_states(collectors, y_c))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on Catastrophe States At Year')\n",
    "\n",
    "def print_catastrophe(collectors, y_c):\n",
    "    catastrophes = [[('', 0)] if len(cs['catastrophe']) == 0 else [(c['catastrophe'], c['year']) for c in cs['catastrophe']] for cs in collectors]\n",
    "    catastrophes = [c  if c[0][1] <= y_c else '' for c in catastrophes]\n",
    "    c = Counter([' '.join(sorted([c[0] for c in cs])) for cs in catastrophes])\n",
    "    c = dict([(k, round(v / VARS['RUNS'] * 100, 2)) for k, v in c.items()])\n",
    "    c = sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "    c = [c_ for c_ in c if c_[1] >= 0.1]\n",
    "    return c\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    pprint(print_catastrophe(collectors, y_c))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on *First* Catastrophe State At Year')\n",
    "\n",
    "def print_catastrophe_first(collectors, y_c):\n",
    "    catastrophes = [[('', 0)] if len(cs['catastrophe']) == 0 else [(c['catastrophe'], c['year']) for c in cs['catastrophe']] for cs in collectors]\n",
    "    c = Counter([c[0][0] if c[0][1] <= y_c else '' for c in catastrophes])\n",
    "    c = dict([(k, round(v / VARS['RUNS'] * 100, 2)) for k, v in c.items()])\n",
    "    c = sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "    c = [c_ for c_ in c if c_[1] >= 0.1]\n",
    "    return c\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    pprint(print_catastrophe_first(collectors, y_c))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on # of Catastrophes At Year')\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## # of catastrophes as of {} ##'.format(y_c))  \n",
    "    pprint(sq.get_percentiles([len([c for c in cs['catastrophe'] if c['year'] <= y_c]) for cs in collectors]))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on Double Catastrophe X-Risks')\n",
    "\n",
    "def print_double_catastrophes(catastrophes):\n",
    "    c = Counter(['' if c is None else c for c in catastrophes])\n",
    "    c = dict([(k, round(v / VARS['RUNS'] * 100, 3)) for k, v in c.items()])\n",
    "    c = sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "    # c = [c_ for c_ in c if c_[1] >= 0.1]\n",
    "    return c\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    pprint(print_double_catastrophes(['' if c['final_year'] > y_c else c['double_catastrophe_xrisk'] for c in collectors]))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_found = False\n",
    "i = 0\n",
    "y_range = list(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))\n",
    "trigger_found = False\n",
    "while not trigger_found and i < len(y_range):\n",
    "    print('.', end='')\n",
    "    y_c = y_range[i]\n",
    "    ai = find_xrisk(y_c, 'tai')\n",
    "    nukes = find_xrisk(y_c, 'nukes')\n",
    "    unknown = find_xrisk(y_c, 'unknown')\n",
    "    nano = find_xrisk(y_c, 'nanotech')\n",
    "    natural = find_xrisk(y_c, 'supervolcano')\n",
    "    bio = find_xrisk(y_c, 'bio')\n",
    "    xrisk = ai + nukes + bio + nano + unknown\n",
    "    if xrisk > 0.05:\n",
    "        trigger_found = True\n",
    "        y_c -= (xrisk - 0.05)/0.05\n",
    "    i += 1\n",
    "\n",
    "print('')\n",
    "if not trigger_found:\n",
    "    y_c = '>{}'.format(VARS['MAX_YEAR'])\n",
    "else:\n",
    "    y_c = dt.datetime.strftime(dt.datetime.now() + dt.timedelta(days=(y_c - 2023) * 365.24), '%Y %b %d')\n",
    "    \n",
    "print('5% X-Risk By {}'.format(y_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total X-Risk IN THAT SPECIFIC YEAR (non-cumulative) (including non-extinction x-risks)')\n",
    "\n",
    "print('1/6: AI')\n",
    "ai = np.diff(np.array([find_xrisk(y, 'tai') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('2/6: Nukes')\n",
    "nukes = np.diff(np.array([find_xrisk(y, 'nukes') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('3/6: Unknown')\n",
    "unknown = np.diff(np.array([find_xrisk(y, 'unknown') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('4/6: Nanotech')\n",
    "nano = np.diff(np.array([find_xrisk(y, 'nanotech') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('5/6: Natural risk')\n",
    "natural = np.diff(np.array([find_xrisk(y, 'supervolcano') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('6/6: Biorisk')\n",
    "bio = np.diff(np.array([find_xrisk(y, 'bio') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "\n",
    "xrisk_df = pd.DataFrame({'year': range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR'] - 1),\n",
    "                         'ai': ai,\n",
    "                         'nukes': nukes,\n",
    "                         'unknown': unknown,\n",
    "                         'nano': nano,\n",
    "                         'natural': natural,\n",
    "                         'bio': bio})\n",
    "xrisk_df['total'] = xrisk_df['ai'] + xrisk_df['nukes'] + xrisk_df['unknown'] + xrisk_df['nano'] + xrisk_df['natural'] + xrisk_df['bio']\n",
    "xrisk_df.to_csv('caches/xrisk_df.csv', index=False)\n",
    "xrisk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for y_c in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR'] - 1)):\n",
    "    state = dict([s for s in print_states(collectors, y_c) if 'morally_good_actually' not in s[0]])\n",
    "    state['year'] = y_c\n",
    "    all_states.append(state)\n",
    "\n",
    "world_states_df = pd.DataFrame(all_states)\n",
    "world_states_df.to_csv('caches/world_states_df.csv', index=False)\n",
    "world_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total X-Risk OR catastrophe by EOY year')\n",
    "\n",
    "for y_c in target_years:\n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: ('xrisk' in e['category'] and e['final_year'] <= y_c) or (len(e['catastrophe']) > 0 and e['catastrophe'][0]['year'] <= y_c),\n",
    "                       n=VARS['RUNS'])\n",
    "    print('{} - {}%'.format(y_c, round(r * 100, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total X-Risk AND catastrophe by EOY year')\n",
    "\n",
    "for y_c in target_years:\n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: ('xrisk' in e['category'] and e['final_year'] <= y_c) and (len(e['catastrophe']) > 0 and e['catastrophe'][0]['year'] <= y_c),\n",
    "                       n=VARS['RUNS'])\n",
    "    print('{} - {}%'.format(y_c, round(r * 100, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://forum.effectivealtruism.org/posts/nYgw4FNpHf9bmJGEi/forecasting-thread-how-does-ai-risk-level-vary-based-on\n",
    "\n",
    "def generate_conditional(y_low, y_high):\n",
    "    def fn(e):\n",
    "        if e['tai_year'] is None:\n",
    "            return False\n",
    "        elif e['tai_year'] < y_low:\n",
    "            return False\n",
    "        elif e['tai_year'] > y_high:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    return fn\n",
    "    \n",
    "\n",
    "def find(y_low, y_high, category):\n",
    "    try:\n",
    "        return bayes.bayesnet(define_event_lambda,\n",
    "                              find=lambda e: e['category'] == category and e['final_year'] <= (VARS['MAX_YEAR'] - 1 if y_high >= VARS['MAX_YEAR'] else y_high),\n",
    "                              conditional_on=generate_conditional(y_low, y_high),\n",
    "                              n=VARS['RUNS'])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "for y_c in [[2022, 2029], [2029, 2039], [2039, 2059], [2059, VARS['MAX_YEAR'] - 1], [2022, 2070]]:\n",
    "    print('AI X-Risk conditional on AGI beween beginning of {} and end of {}'.format(y_c[0] + 1, y_c[1]))\n",
    "    extinction = find(y_c[0], y_c[1], 'xrisk_full_unaligned_tai_extinction')\n",
    "    singleton = find(y_c[0], y_c[1], 'xrisk_full_unaligned_tai_singleton')\n",
    "    subtle_misalignment = find(y_c[0], y_c[1], 'xrisk_subtly_unaligned_tai')\n",
    "    misuse = find(y_c[0], y_c[1], 'xrisk_tai_misuse')\n",
    "    out = '{}% (Extinction: {}%, Bad TAI singleton: {}%, Subtly misaligned singleton: {}%, Misuse singleton: {}%)'\n",
    "    print(out.format(round((extinction + singleton + subtle_misalignment + misuse) * 100, 1),\n",
    "                     round(extinction * 100, 1),\n",
    "                     round(singleton * 100, 1),\n",
    "                     round(subtle_misalignment * 100, 1),\n",
    "                     round(misuse * 100, 1)))\n",
    "    print('-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Have we seen the following wars by EOY year?')\n",
    "\n",
    "def print_wars(wars, y_c):\n",
    "    bs = [[w['belligerents'] for w in ws if w != [] and w['start_year'] <= y_c] for ws in wars]\n",
    "    bs = Counter([' '.join(sorted(b)) for b in bs])    \n",
    "    bs = dict([(k, round(v / VARS['RUNS'] * 100, 3)) for k, v in bs.items()])\n",
    "    bs = sorted(bs.items(), key=lambda x: x[1], reverse=True)\n",
    "    for war in ['US/China', 'US/Russia', 'Other']:\n",
    "        print('{}: {}%'.format(war, round(sum([b[1] if war in b[0] else 0 for b in bs]), 1)))\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    print_wars([c['wars'] for c in collectors], y_c)\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Offensive nuclear weapon use (1+ fatality) by EOY year?')\n",
    "\n",
    "def p_nuke_used_by(y_c):\n",
    "    return bayes.bayesnet(define_event_lambda,\n",
    "                          find=lambda e: len(e['nuclear_weapon_used']) > 0 and e['nuclear_weapon_used'][0] <= y_c,\n",
    "                          n=VARS['RUNS'])\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('{}: {}%'.format(y_c, round(p_nuke_used_by(y_c) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on war states')\n",
    "\n",
    "def print_wars(wars, y_c):\n",
    "    bs = [[w['belligerents'] for w in ws if w != [] and w['start_year'] <= y_c] for ws in wars]\n",
    "    bs = Counter([' '.join(sorted(b)) for b in bs])    \n",
    "    bs = dict([(k, round(v / VARS['RUNS'] * 100, 3)) for k, v in bs.items()])\n",
    "    return sorted(bs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    pprint(print_wars([c['wars'] for c in collectors], y_c))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on # of Wars At Year')\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## # of wars as of {} ##'.format(y_c))  \n",
    "    pprint(sq.get_percentiles([len([c for c in cs['wars'] if c['start_year'] <= y_c]) for cs in collectors]))\n",
    "    print('-')\n",
    "    print('-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on War Length States At Year')\n",
    "\n",
    "def print_wars(wars, y_c):\n",
    "    bs = [[(w['end_year'] - w['start_year'] if w['end_year'] < y_c else y - w['start_year']) if w != [] and w['start_year'] <= y_c else 0 for w in ws] for ws in wars]\n",
    "    bs = [round(sum(b) / (y - VARS['CURRENT_YEAR']) * 100, 1) for b in bs]\n",
    "    return bs\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## Percent of time in war as of {} ##'.format(y_c))  \n",
    "    pprint(sq.get_percentiles(print_wars([c['wars'] for c in collectors], y_c)))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
