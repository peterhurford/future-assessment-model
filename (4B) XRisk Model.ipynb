{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1\n",
      "Loaded 2\n",
      "Using squiggle version 0.28-dev1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import dill\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import squigglepy as sq\n",
    "\n",
    "from squigglepy import bayes\n",
    "from squigglepy.numbers import K, M, B, T\n",
    "from copy import deepcopy\n",
    "from scipy import stats\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime as dt, timedelta\n",
    "\n",
    "%matplotlib inline\n",
    "print('Loaded 1')\n",
    "\n",
    "exec(open('utils.py').read())\n",
    "print('Loaded 2')\n",
    "\n",
    "print('Using squiggle version {}'.format(sq.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "VARS = {}\n",
    "\n",
    "# Global variables - probably don't want to change these but you could.\n",
    "VARS['RUNS'] = 100_000                                  # Number of runs to do (default 100*K)\n",
    "VARS['CURRENT_YEAR'] = 2024                             # What year to start the run on? (default: 2024)\n",
    "VARS['MAX_YEAR'] = VARS['CURRENT_YEAR'] + 100           # What year to end the run on? (default: 2124)\n",
    "\n",
    "CURRENT_YEAR = VARS['CURRENT_YEAR']\n",
    "years = range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR'])\n",
    "print('Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAI Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TAI scenarios module\n"
     ]
    }
   ],
   "source": [
    "# Conditional on making agentic TAI, will it be aligned by default? Or will it otherwise fail to be power-seeking in a relevant way?\n",
    "VARS['p_tai_alignment_is_easy'] = 0.3\n",
    "VARS['p_tai_not_powerseeking'] = 0.3\n",
    "VARS['p_tai_aligned_by_default'] = VARS['p_tai_alignment_is_easy'] + ((1 - VARS['p_tai_alignment_is_easy']) * VARS['p_tai_not_powerseeking'])\n",
    "\n",
    "\n",
    "# Conditional on making agentic TAI that is not aligned by default, will we solve the alignment problem?\n",
    "# Varies by year, whether this is the first attempt, and whether there is a great power war\n",
    "def p_alignment_solved(year, first_attempt=True, verbose=False):\n",
    "    if first_attempt:\n",
    "        p = min(2.2 * (year/45), 0.8)\n",
    "    else:\n",
    "        p = min(2.8 * (year/45), 0.9)\n",
    "    if verbose == 2:\n",
    "        print('* alignment solve diagnostic - war: {} year: {} first attempt: {} -> p {})'.format(war, year, first_attempt, p))\n",
    "    return p\n",
    "VARS['p_alignment_solved'] = p_alignment_solved\n",
    "# TODO: Convert to logistic curves\n",
    "\n",
    "\n",
    "# Conditional on making agentic TAI that is not aligned by default, will we successfully coordinate to deploy safe AI?\n",
    "# Varies by year, whether this is the first attempt, and whether there is a great power war\n",
    "def p_alignment_deployment_safety_and_coordination(year, war, variables, first_attempt=True, verbose=False):\n",
    "    if year <= 2030 - variables['CURRENT_YEAR']:\n",
    "        p = 0.7 if war else 0.85\n",
    "    else:\n",
    "        p = 0.4 if war else 0.6\n",
    "    if first_attempt is False:\n",
    "        p += 0.1\n",
    "    p = min(p, 0.95)\n",
    "    p = max(p, 0.05)\n",
    "    if verbose == 2:\n",
    "        print('* alignment coordination diagnostic - war: {} year: {} first attempt: {} -> p {})'.format(war, year, first_attempt, p))\n",
    "    return p\n",
    "VARS['p_alignment_deploy_coordination'] = p_alignment_deployment_safety_and_coordination\n",
    "\n",
    "# Conditional on solving the alignment problem, what is the chance we also solve the subtle misalignment problem?\n",
    "VARS['p_subtle_alignment_solved'] = 0.85\n",
    "\n",
    "# Conditional on alignment by default, what is the chance we also solve the subtle misalignment problem?\n",
    "VARS['p_subtle_alignment_solved_if_aligned_by_default'] = 0.4\n",
    "\n",
    "# Conditional on having aligned AI, will we know it is aligned and therefore want to deploy it?\n",
    "VARS['p_know_aligned_ai_is_aligned'] = 0.6\n",
    "\n",
    "# Conditional on having misaligned AI, will we know it is misaligned and therefore not want to deploy it?\n",
    "VARS['p_know_misaligned_ai_is_misaligned'] = 0.7\n",
    "\n",
    "# Conditional on having agentic TAI, will it be intentionally misused to create a singleton?\n",
    "def p_tai_intentional_misuse(war):\n",
    "    return 0.3 if war else 0.05\n",
    "VARS['p_tai_intentional_misuse'] = p_tai_intentional_misuse\n",
    "\n",
    "# If TAI is fully misaligned what is the chance we can successfully detect and avert this?\n",
    "VARS['p_full_tai_misalignment_averted'] = 0.2\n",
    "\n",
    "# If TAI is fully misaligned but successfully averted, what is the probability there will be a catasrophe (10%+ death)?\n",
    "VARS['p_tai_misalignment_averting_is_catastrophic'] = 0.4\n",
    "\n",
    "# If TAI is fully misaligned and we successfully avert it, what is the chance we give up on TAI?\n",
    "VARS['p_full_tai_misalignment_averted_means_abandoned_tai'] = 0.7\n",
    "\n",
    "# If TAI is fully misaligned, what is the chance it results in extinction versus a singleton?\n",
    "VARS['p_tai_xrisk_is_extinction'] = 0.1\n",
    "\n",
    "# If there is a fully misaligned TAI singleton, what is the chance it results in a non-extinction catastrophe (10%+ death)?\n",
    "VARS['p_tai_singleton_is_catastrophic'] = 0.8\n",
    "\n",
    "# If ther is an intentional attempt to misuse TAI to create a singleton, what is the chance it causes extinction instead?\n",
    "VARS['p_intentional_tai_singleton_creates_extinction'] = 0.05\n",
    "\n",
    "\n",
    "exec(open('modules/tai_risk.py').read())\n",
    "print('Loaded TAI scenarios module')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache from: 2024-04-26 10:13:32.481876\n",
      "Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'algorithms': {'prob': <function __main__.derive_nonscaling_delay_curve.<locals>.p_nonscaling_delay(year)>,\n",
       "  'length': <Distribution> mixture\n",
       "   - 0.9 weight on <Distribution> lognorm(lognorm_mean=3.29, lognorm_sd=0.93, norm_mean=1.15, norm_sd=0.28)\n",
       "   - 0.1 weight on <Distribution> lognorm(lognorm_mean=8.08, lognorm_sd=6.42, norm_mean=1.84, norm_sd=0.7) (version 0.28-dev1)},\n",
       " 'infra': {'prob': <function __main__.derive_nonscaling_delay_curve.<locals>.p_nonscaling_delay(year)>,\n",
       "  'length': <Distribution> lognorm(lognorm_mean=2.52, lognorm_sd=1.31, norm_mean=0.8, norm_sd=0.49) (version 0.28-dev1)},\n",
       " 'data': {'prob': <function __main__.derive_nonscaling_delay_curve.<locals>.p_nonscaling_delay(year)>,\n",
       "  'length': <Distribution> mixture\n",
       "   - 0.9 weight on <Distribution> lognorm(lognorm_mean=3.45, lognorm_sd=2.42, norm_mean=1.04, norm_sd=0.63)\n",
       "   - 0.1 weight on <Distribution> lognorm(lognorm_mean=5.86, lognorm_sd=2.22, norm_mean=1.7, norm_sd=0.37) (version 0.28-dev1)},\n",
       " 'robotics': {'prob': <function __main__.derive_nonscaling_delay_curve.<locals>.p_nonscaling_delay(year)>,\n",
       "  'length': <Distribution> mixture\n",
       "   - 0.85 weight on <Distribution> lognorm(lognorm_mean=2.52, lognorm_sd=1.31, norm_mean=0.8, norm_sd=0.49)\n",
       "   - 0.15 weight on <Distribution> lognorm(lognorm_mean=5.43, lognorm_sd=5.35, norm_mean=1.35, norm_sd=0.82) (version 0.28-dev1)},\n",
       " 'integration': {'prob': <function __main__.derive_nonscaling_delay_curve.<locals>.p_nonscaling_delay(year)>,\n",
       "  'length': <Distribution> lognorm(lognorm_mean=12.12, lognorm_sd=9.64, norm_mean=2.25, norm_sd=0.7) (version 0.28-dev1)},\n",
       " 'real_world_feedback': {'prob': <function __main__.derive_nonscaling_delay_curve.<locals>.p_nonscaling_delay(year)>,\n",
       "  'length': <Distribution> mixture\n",
       "   - 0.8 weight on <Distribution> lognorm(lognorm_mean=5.86, lognorm_sd=2.22, norm_mean=1.7, norm_sd=0.37)\n",
       "   - 0.1 weight on <Distribution> lognorm(lognorm_mean=18.31, lognorm_sd=6.29, norm_mean=2.85, norm_sd=0.33)\n",
       "   - 0.1 weight on <Distribution> lognorm(lognorm_mean=32.87, lognorm_sd=9.34, norm_mean=3.45, norm_sd=0.28) (version 0.28-dev1)},\n",
       " 'take_off': {'prob': <function __main__.derive_nonscaling_delay_curve.<locals>.p_nonscaling_delay(year)>,\n",
       "  'length': <Distribution> lognorm(lognorm_mean=6.61, lognorm_sd=4.46, norm_mean=1.7, norm_sd=0.61) (version 0.28-dev1)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBABILITY OF A NONSCALING DELAY\n",
    "\n",
    "# set to None to have no delay\n",
    "# Otherwise specified in a dictionary\n",
    "# {'delay': {'prob': <array of probabilities by year>, 'length': <distribution to sample from to get length of delay>}}\n",
    "\n",
    "# Cache defined in \"(3B) Nonscaling Delay Curve\"\n",
    "\n",
    "with open('caches/nonscaling_delays.dill', 'rb') as f:\n",
    "    delay = dill.load(f)\n",
    "\n",
    "print('Cache from: {}'.format(dt.fromtimestamp(os.path.getmtime('caches/nonscaling_delays.dill'))))\n",
    "VARS['delay'] = delay\n",
    "print('Loaded')\n",
    "delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuclear Scenarios Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nuclear scenarios module\n"
     ]
    }
   ],
   "source": [
    "# Annual chance Russia uses a nuke\n",
    "def p_russia_uses_nuke(peace, year, variables):\n",
    "    peace = 10 if peace else 1\n",
    "    year = year + variables['CURRENT_YEAR']\n",
    "    if year == 2023:\n",
    "        return 0.03\n",
    "    else:\n",
    "        return 0.001 / peace\n",
    "VARS['p_russia_uses_nuke'] = p_russia_uses_nuke\n",
    "\n",
    "\n",
    "# Annual chance North Korea uses a nuke\n",
    "VARS['p_nk_uses_nuke'] = 0.001\n",
    "\n",
    "\n",
    "# Annual chance China uses a nuke\n",
    "def p_china_uses_nuke(peace, year, variables):\n",
    "    return VARS['p_china_invades_taiwan'](peace, year, variables) * 0.01\n",
    "VARS['p_china_uses_nuke'] = p_china_uses_nuke\n",
    "\n",
    "    \n",
    "# Annual chance another country uses a nuke\n",
    "def p_other_uses_nuke(peace):\n",
    "    peace = 10 if peace else 1\n",
    "    return 0.0002 / peace\n",
    "VARS['p_other_uses_nuke'] = p_other_uses_nuke\n",
    "\n",
    "\n",
    "# What is the chance in a given year there will be a \"nuclear accident\"?\n",
    "def p_nuclear_accident(war, year):\n",
    "    p = 0.05 if war else 0.02\n",
    "    p = p * (0.998 ** year)\n",
    "    return p\n",
    "VARS['p_nuclear_accident'] = p_nuclear_accident\n",
    "\n",
    "\n",
    "# Conditional on a nuclear accident, what is the chance it escalates into an \"exchange\"?\n",
    "def p_nuclear_accident_becomes_exchange(war):\n",
    "    return 0.2 if war else 0.05\n",
    "VARS['p_nuclear_accident_becomes_exchange'] = p_nuclear_accident_becomes_exchange\n",
    "\n",
    "\n",
    "# Conditional on a nuclear exchange, what is the chance it escalates into a catastrophe (10%+ dead)?\n",
    "def p_catastrophe_from_nuclear_exchange(war):\n",
    "    p_exchange_becomes_all_out_war = 0.6 if war else 0.3\n",
    "    p_nuclear_winter_happens = 0.4 # Also something about ozone?\n",
    "    alternative_foods_or_other_save = 0.1\n",
    "    return (p_exchange_becomes_all_out_war *\n",
    "            (p_nuclear_winter_happens + (1 - p_nuclear_winter_happens) * 0.1) *\n",
    "            (1 - alternative_foods_or_other_save))\n",
    "VARS['p_catastrophe_from_nuclear_exchange'] = p_catastrophe_from_nuclear_exchange\n",
    "\n",
    "    \n",
    "# Conditional on a nuclear exchange catastrophe, what is the chance it becomes an xrisk?\n",
    "VARS['p_xrisk_from_nuclear_catastrophe'] = 0.05 # https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\n",
    "\n",
    "\n",
    "# Conditional on a great power war, what is the chance it goes intentionally nuclear in any given year?\n",
    "def p_nuclear_exchange_given_war(first_year_of_war):\n",
    "    return 0.1 if first_year_of_war else 0.02\n",
    "VARS['p_nuclear_exchange_given_war'] = p_nuclear_exchange_given_war\n",
    "\n",
    "\n",
    "exec(open('modules/nuclear.py').read())\n",
    "print('Loaded nuclear scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China Scenarios Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(11480 - China invade Taiwan before 2035) = 50.0%\n",
      "p(12309 - China blockade Taiwan before 2035) = 60.0%\n",
      "p(11112 - US defends conditional) = 82.0%\n",
      "p(7812 - Anyone defends conditional) = 85.2%\n",
      "p(5320 - China half of Taiwan) = 22.7%\n",
      "p(8362 - US-China war) = 45.3%\n",
      "p(12795 - Taiwan independence) = 55.2%\n",
      "p(12794 - US ambassador) = 40.8%\n"
     ]
    }
   ],
   "source": [
    "# Will China launch a full-scale invasion of Taiwan by the following years?\n",
    "# https://www.metaculus.com/questions/11480/china-launches-invasion-of-taiwan/\n",
    "p__china_invade_before_2025 = 0.01\n",
    "p__china_invade_before_2030 = 0.4\n",
    "p__china_invade_before_2035 = 0.5\n",
    "print('p(11480 - China invade Taiwan before 2035) = {}%'.format(round(p__china_invade_before_2035 * 100, 1)))\n",
    "\n",
    "\n",
    "# Will China engage in a full-scale blockade against Taiwan before the following years?\n",
    "# https://www.metaculus.com/questions/12309/china-engages-in-a-blockade-against-taiwan/\n",
    "p__china_blockade_before_2025 = 0.05\n",
    "p__china_blockade_before_2030 = 0.5\n",
    "p__china_blockade_before_2035 = 0.6\n",
    "print('p(12309 - China blockade Taiwan before 2035) = {}%'.format(round(p__china_blockade_before_2035 * 100, 1)))\n",
    "\n",
    "\n",
    "# If China invades Taiwan before 2035, will the US respond with military force?\n",
    "# https://www.metaculus.com/questions/11112/us-response-if-china-invades-taiwan-2035/\n",
    "p__us_protects_voluntarily = 0.4\n",
    "p__china_preemptive_attack = 0.4\n",
    "p__us_defends_japan = 0.5\n",
    "\n",
    "p__us_defends = (p__china_preemptive_attack +\n",
    "                (1 - p__china_preemptive_attack) * p__us_protects_voluntarily +\n",
    "                (1 - p__china_preemptive_attack) * (1 - p__us_protects_voluntarily) * p__us_defends_japan)\n",
    "print('p(11112 - US defends conditional) = {}%'.format(round(p__us_defends * 100, 1)))\n",
    "\n",
    "\n",
    "# If there are 100 deaths in conflict between China and Taiwan before 2050, will Taiwan receive direct military support from allied nations?\n",
    "# https://www.metaculus.com/questions/7812/taiwan-to-receive-support-in-china-conflict/\n",
    "print('p(7812 - Anyone defends conditional) = {}%'.format(round((p__us_defends + (1 - p__us_defends) * 0.6 * 0.3) * 100, 1)))\n",
    "\n",
    "\n",
    "# Will the People's Republic of China annex at least half of Taiwan before 2050?\n",
    "# https://www.metaculus.com/questions/5320/chinese-annexation-of-half-of-taiwan-by-2050/\n",
    "p__china_half_of_taiwan = p__china_invade_before_2035 * (0.7 * (1 - p__us_defends) + 0.4 * p__us_defends)\n",
    "print('p(5320 - China half of Taiwan) = {}%'.format(round(p__china_half_of_taiwan * 100, 1)))\n",
    "\n",
    "\n",
    "# Will there be a US-China war before 2035?\n",
    "# https://www.metaculus.com/questions/8362/us-china-war-before-2035/\n",
    "p__us_china_war = (p__china_invade_before_2035 * p__us_defends +\n",
    "                  (1 - p__china_invade_before_2035) * (p__china_blockade_before_2035) * p__us_defends * 0.7 * 0.25)\n",
    "print('p(8362 - US-China war) = {}%'.format(round(p__us_china_war * 100, 1)))\n",
    "\n",
    "\n",
    "# Will Taiwan/Republic of China declare independence by 2035?\n",
    "# https://www.metaculus.com/questions/12795/taiwan-declares-independence-by-2035/\n",
    "p__taiwan_independence = (p__china_invade_before_2035 * 0.9 +\n",
    "                         (1 - p__china_invade_before_2035) * (p__china_blockade_before_2035) * 0.8 * 0.25 +\n",
    "                         (1 - p__china_invade_before_2035) * (1 - p__china_blockade_before_2035 * 0.25) * 0.1)\n",
    "print('p(12795 - Taiwan independence) = {}%'.format(round(p__taiwan_independence * 100, 1)))\n",
    "\n",
    "\n",
    "# If Taiwan declares independence by 2035, will the United States appoint an ambassador within a year?\n",
    "# https://www.metaculus.com/questions/12794/us-recognizes-roc-independence-declaration/\n",
    "p__us_ambassador = (p__china_invade_before_2035 * 0.9 * 0.8 +\n",
    "                    (1 - p__china_invade_before_2035) * (p__china_blockade_before_2035) * 0.8 * 0.25 * 0.8)\n",
    "print('p(12794 - US ambassador) = {}%'.format(round(p__us_ambassador * 100, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Power War Scenarios Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded great power war scenarios module\n"
     ]
    }
   ],
   "source": [
    "# Annual chance that, conditional on Russia not nuking outside of a great power war, there will be a US-Russia great power war\n",
    "def p_great_power_war_us_russia_without_nuke_first(peace, year, variables):\n",
    "    peace = 20 if peace else 1\n",
    "    year = year + variables['CURRENT_YEAR']\n",
    "    if year <= 2025:\n",
    "        return (0.01 / 2) / peace\n",
    "    else:\n",
    "        return 0.002 / peace\n",
    "VARS['p_great_power_war_us_russia_without_nuke_first'] = p_great_power_war_us_russia_without_nuke_first\n",
    "\n",
    "\n",
    "def p_china_invades_taiwan(peace, year, variables):\n",
    "    peace = 10 if peace else 1\n",
    "    year = year + variables['CURRENT_YEAR']\n",
    "    if year == 2024 or year == 2025:\n",
    "        return 0.01\n",
    "    elif year < 2030:\n",
    "        return 0.088 # makes cumulative probability by EOY 2029 = 0.4\n",
    "                     # solve 0.05 + (1-0.05)*X + (1-0.05)(1-X)*X + (1-0.05)(1-X)^2*X + (1-0.05)(1-X)^3*X + (1-0.06)(1-X)^4*X = 0.4\n",
    "    elif year < 2035:\n",
    "        return 0.0358  # makes cumulative probability by EOY 2034 = 0.5\n",
    "                        # 0.4 + (1-0.4)*X + (1-0.4)(1-X)*X + (1-0.4)(1-X)^2*X + (1-0.4)(1-X)^3*X + (1-0.4)(1-X)^4*X = 0.5\n",
    "    else:\n",
    "        return 0.005 / peace\n",
    "VARS['p_china_invades_taiwan'] = p_china_invades_taiwan\n",
    "\n",
    "    \n",
    "# Annual chance that there is a great power war between the US and China\n",
    "def p_great_power_war_us_china(peace, year, variables):\n",
    "    p_invade_taiwan = variables['p_china_invades_taiwan'](peace, year, variables)\n",
    "    p_us_responds = 0.82\n",
    "    return p_invade_taiwan * p_us_responds\n",
    "VARS['p_great_power_war_us_china'] = p_great_power_war_us_china\n",
    "\n",
    "    \n",
    "# Annual chance that there is some great power war other than US<>Russia and US<>China\n",
    "def p_great_power_war_other(peace, year, variables):\n",
    "    peace = 5 if peace else 1\n",
    "    year = year + variables['CURRENT_YEAR']\n",
    "    if year > 2040:\n",
    "        return 0.003 / peace\n",
    "    else:\n",
    "        return 0.0005 / peace\n",
    "VARS['p_great_power_war_other'] = p_great_power_war_other\n",
    "\n",
    "\n",
    "# Conditional on a great power war starting, how long will it last?\n",
    "VARS['war_length'] = sq.lognorm(2, 50) # 90% CI\n",
    "\n",
    "# After a war ends, how long will there be a peace?\n",
    "VARS['peace_length'] = sq.lognorm(10, 100)\n",
    "\n",
    "\n",
    "exec(open('modules/great_power_war.py').read())\n",
    "print('Loaded great power war scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio scenarios module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bio scenarios module\n"
     ]
    }
   ],
   "source": [
    "# Conditional on a great power war, what is the annual chance it intentionally results in a bioweapon?\n",
    "VARS['p_biowar_given_war'] = 1/800\n",
    "\n",
    "# What is the annual chance of a non-state actor creating an intentional biorisk that causes 1%+ death?\n",
    "VARS['p_nonstate_bio'] = 1/600\n",
    "\n",
    "# What is the chance that if 1%+ die from natural bio, 10%+ will die from natural bio?\n",
    "VARS['p_natural_bio_is_catastrophe'] = 1 / (10 ** 0.5) # https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028\n",
    "\n",
    "# What is the chance that if 1%+ die from engineered bio, 10%+ will die from engineered bio?\n",
    "VARS['p_engineered_bio_is_catastrophe'] = 1 / (10 ** 0.5) # https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028\n",
    "\n",
    "VARS['p_covid_spanish_flu_like_becomes_1pct_death'] = 1 / (10 ** 0.5) # https://www.economist.com/graphic-detail/coronavirus-excess-deaths-estimates suggests COVID killed 0.2%... https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028 suggests 1% is 2x less likely than 0.2%\n",
    "VARS['p_covid_lab_leak'] = 0.1\n",
    "VARS['p_extinction_given_90_pct_death'] = 0.03 # per Luisa https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would\n",
    "VARS['p_accidental_catastrophe_causes_90_pct_death'] = (1 / (10 ** 0.5)) ** 2 # https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028\n",
    "VARS['p_intentional_catastrophe_causes_90_pct_death'] = (1 / (10 ** 0.5)) ** 2 # https://www.liebertpub.com/doi/pdfplus/10.1089/hs.2017.0028\n",
    "\n",
    "# If a lab leak occurs, how likely is it that the leaked pandemic will be engineered vs. natural?\n",
    "VARS['ratio_engineered_vs_natural_lab_leak'] = 0.8\n",
    "\n",
    "# What is the chance of an natural biorisk/pandemic causing 1%+ population death?\n",
    "def p_natural_bio(year, variables):\n",
    "    base_rate_from_covid_and_spanish_flu = 1/250\n",
    "    increase_from_globalization = 1.1\n",
    "    decreate_in_rate_per_year_from_improvements = 0.99 ** year\n",
    "    return ((base_rate_from_covid_and_spanish_flu * 0.5 +\n",
    "             base_rate_from_covid_and_spanish_flu * (1 - variables['p_covid_lab_leak']) * 0.5) *\n",
    "            increase_from_globalization *\n",
    "            variables['p_covid_spanish_flu_like_becomes_1pct_death'] *\n",
    "            decreate_in_rate_per_year_from_improvements)\n",
    "VARS['p_natural_bio'] = p_natural_bio\n",
    "    \n",
    "    \n",
    "# What is the chance of an accidental biorisk (e.g., lab leak) causing 1%+ population death?\n",
    "def p_accidental_bio(war, variables):\n",
    "    base_rate_from_covid = 0.01 * variables['p_covid_lab_leak']\n",
    "    increase_factor_due_to_increasing_labs = 1.3\n",
    "    increase_factor_due_to_great_power_war = 2\n",
    "    p = (base_rate_from_covid *\n",
    "         variables['p_covid_spanish_flu_like_becomes_1pct_death'] *\n",
    "         increase_factor_due_to_increasing_labs)\n",
    "    return p * increase_factor_due_to_great_power_war if war else p\n",
    "VARS['p_accidental_bio'] = p_accidental_bio    \n",
    "\n",
    "\n",
    "# Conditional on a accidental biorisk (1% death), what is the chance it becomes a xrisk?\n",
    "def p_xrisk_from_accidental_bio_given_catastrophe(year, variables):\n",
    "    return variables['p_accidental_catastrophe_causes_90_pct_death'] * variables['p_extinction_given_90_pct_death']\n",
    "VARS['p_xrisk_from_accidental_bio_given_catastrophe'] = p_xrisk_from_accidental_bio_given_catastrophe\n",
    "\n",
    "\n",
    "# Conditional on a bioweapon, what is the chance it becomes a xrisk?\n",
    "def p_xrisk_from_engineered_bio_given_catastrophe(year, variables):\n",
    "    return variables['p_intentional_catastrophe_causes_90_pct_death'] * variables['p_extinction_given_90_pct_death']\n",
    "VARS['p_xrisk_from_engineered_bio_given_catastrophe'] = p_xrisk_from_engineered_bio_given_catastrophe\n",
    "\n",
    "\n",
    "exec(open('modules/bio.py').read())\n",
    "print('Loaded bio scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nanotech scenarios module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nano scenarios module\n"
     ]
    }
   ],
   "source": [
    "# What is the chance in a given year that nanotech will be developed?\n",
    "def p_nanotech_possible(year):\n",
    "    if year < 200:\n",
    "        return 0.0001 / (0.956 ** year)\n",
    "    else:\n",
    "        return 1 # TODO: This is dumb\n",
    "VARS['p_nanotech_possible'] = p_nanotech_possible\n",
    "\n",
    "\n",
    "# Conditional on developing nanotech, what is the chance nanotech results in an xrisk?\n",
    "VARS['p_nanotech_is_xrisk'] = 0.1 * 0.05\n",
    "\n",
    "\n",
    "exec(open('modules/nano.py').read())\n",
    "print('Loaded nano scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervolcano scenarios module (all other natural risks <0.01%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded supervolcano module\n"
     ]
    }
   ],
   "source": [
    "VARS['p_supervolcano_catastrophe'] = 1 / (500*K)  # https://www.openphilanthropy.org/research/large-volcanic-eruptions/ VEI >= 9 (geometric mean of 30K and 30M)\n",
    "\n",
    "VARS['p_supervolcano_extinction_given_catastrophe'] = 0.05\n",
    "\n",
    "\n",
    "exec(open('modules/supervolcano.py').read())\n",
    "print('Loaded supervolcano module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown unknown scenarios module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded unknown unknown scenarios module\n"
     ]
    }
   ],
   "source": [
    "# What is the chance in any given year that an unknown unknown xrisk occurs?\n",
    "def p_unknown_unknown_xrisk(year):\n",
    "    return 1 / (100*K) / (0.99 ** min(230, year)) # TODO: This is dumb\n",
    "VARS['p_unknown_unknown_xrisk'] = p_unknown_unknown_xrisk\n",
    "\n",
    "\n",
    "exec(open('modules/unknown_unknown.py').read())\n",
    "print('Loaded unknown unknown scenarios module')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double dip catastrophe module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded double dip catastrophe module\n"
     ]
    }
   ],
   "source": [
    "VARS['p_extinction_from_double_catastrophe'] = 0.1\n",
    "\n",
    "VARS['extinction_from_double_catastrophe_range'] = 10\n",
    "\n",
    "exec(open('modules/double_dip_catastrophe.py').read())\n",
    "print('Loaded double dip catastrophe module')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Timeline variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TAI timelines module\n",
      "-\n",
      "Loading from cache file (`caches/tai_years.sqcache`)...\n",
      "...Loaded\n",
      "Caching in-memory...\n",
      "...Cached!\n",
      "...Reducing\n",
      "...Reduced!\n",
      "...All done!\n",
      "Cache from: 2024-04-26 10:17:32.328449\n",
      "-\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "VARS['if_catastrophe_delay_tai_arrival_by_years'] = sq.lognorm(3, 15)\n",
    "\n",
    "VARS['if_us_china_war_delay_tai_arrival_by_years'] = sq.lognorm(3, 15)\n",
    "\n",
    "exec(open('modules/tai_timelines.py').read())\n",
    "print('Loaded TAI timelines module')\n",
    "\n",
    "print('-')\n",
    "VARS['tai_years'] = bayes.bayesnet(load_cache_file='caches/tai_years', verbose=True)\n",
    "print('Cache from: {}'.format(dt.fromtimestamp(os.path.getmtime('caches/tai_years.sqcache'))))\n",
    "\n",
    "print('-')\n",
    "print('Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded time of perils module\n"
     ]
    }
   ],
   "source": [
    "VARS['tai_ends_time_of_perils'] = 0.1\n",
    "\n",
    "VARS['extinction_is_morally_good_actually'] = 0\n",
    "\n",
    "VARS['misaligned_tai_takeover_is_still_morally_fine'] = 0.1\n",
    "\n",
    "exec(open('modules/time_of_perils.py').read())\n",
    "print('Loaded time of perils module')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached variables!\n"
     ]
    }
   ],
   "source": [
    "with open('caches/variables.dill', 'wb') as f:\n",
    "    dill.dump(VARS, f)\n",
    "print('cached variables!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "## RUN 1 ##\n",
      "############\n",
      "-- sampling p_tai_aligned_by_default p=0.51 outcome=True\n",
      "2031: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 2 years (total delay 2 years)\n",
      "2039: ...catastrophe from engineered pathogen\n",
      "...catastrophe delays TAI by 3 years (total delay 5 years)\n",
      "2045: War ends :)\n",
      "--- /!\\ TAI CREATED in 2070\n",
      "-- sampling p_tai_intentional_misuse p=0.05 outcome=False\n",
      "-- sampling p_alignment_solved p=0.8 outcome=True\n",
      "-- sampling p_subtle_alignment_solved p=0.85 outcome=True\n",
      "-- sampling p_know_aligned_ai_is_aligned p=0.6 outcome=False\n",
      "-- sampling p_alignment_deploy_coordination p=0.7 outcome=True\n",
      "2071: ...coordinated to not deploy TAI\n",
      "-- sampling p_tai_intentional_misuse p=0.05 outcome=True\n",
      "-- sampling p_full_tai_misalignment_averted p=0.2 outcome=True\n",
      "-- sampling p_tai_misalignment_averting_is_catastrophic p=0.4 outcome=False\n",
      "-- sampling p_full_tai_misalignment_averted_means_abandoned_tai p=0.7 outcome=True\n",
      "2072: ...Intentional misuse of TAI happened, it was averted with no catastrophe, and we abandon TAI\n",
      "2102: WAR!!! (US vs. China)\n",
      "2104: War ends :)\n",
      "...Total loop complete in 35.98ms\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 2 ##\n",
      "############\n",
      "-- sampling p_tai_aligned_by_default p=0.51 outcome=False\n",
      "2026: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 13 years (total delay 13 years)\n",
      "2053: War ends :)\n",
      "2122: WAR!!! (US vs. China)\n",
      "...Total loop complete in 15.51ms\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 3 ##\n",
      "############\n",
      "-- sampling p_tai_aligned_by_default p=0.51 outcome=False\n",
      "2059: WAR!!! (Other)\n",
      "2072: War ends :)\n",
      "--- /!\\ TAI CREATED in 2108\n",
      "-- sampling p_tai_intentional_misuse p=0.05 outcome=False\n",
      "-- sampling p_alignment_solved p=0.8 outcome=True\n",
      "-- sampling p_subtle_alignment_solved p=0.85 outcome=True\n",
      "-- sampling p_know_aligned_ai_is_aligned p=0.6 outcome=False\n",
      "-- sampling p_alignment_deploy_coordination p=0.7 outcome=True\n",
      "2109: ...coordinated to not deploy TAI\n",
      "-- sampling p_tai_intentional_misuse p=0.05 outcome=False\n",
      "-- sampling p_alignment_solved p=0.8 outcome=True\n",
      "-- sampling p_subtle_alignment_solved p=0.85 outcome=True\n",
      "-- sampling p_know_aligned_ai_is_aligned p=0.6 outcome=True\n",
      "2110: ...Achieved aligned TAI (aligned via work, first attempt)\n",
      "-- sampling tai_ends_time_of_perils p=0.1 outcome=False\n",
      "2110: Aligned TAI does not end time of perils\n",
      "...Total loop complete in 15.4ms\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 4 ##\n",
      "############\n",
      "-- sampling p_tai_aligned_by_default p=0.51 outcome=True\n",
      "2029: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 11 years (total delay 11 years)\n",
      "2052: War ends :)\n",
      "--- /!\\ TAI CREATED in 2060\n",
      "-- sampling p_tai_intentional_misuse p=0.05 outcome=False\n",
      "-- sampling p_alignment_solved p=0.8 outcome=True\n",
      "-- sampling p_subtle_alignment_solved p=0.85 outcome=True\n",
      "-- sampling p_know_aligned_ai_is_aligned p=0.6 outcome=True\n",
      "2061: ...Achieved aligned TAI (aligned by default)\n",
      "-- sampling tai_ends_time_of_perils p=0.1 outcome=False\n",
      "2061: Aligned TAI does not end time of perils\n",
      "...Total loop complete in 14.17ms\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 5 ##\n",
      "############\n",
      "-- sampling p_tai_aligned_by_default p=0.51 outcome=True\n",
      "2030: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 9 years (total delay 9 years)\n",
      "2033: War ends :)\n",
      "2112: WAR!!! (US vs. Russia)\n",
      "...Total loop complete in 34.95ms\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 6 ##\n",
      "############\n",
      "2027: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 8 years (total delay 8 years)\n",
      "2048: War ends :)\n",
      "--- /!\\ TAI CREATED in 2062\n",
      "2063: ...Achieved aligned TAI (aligned by default)\n",
      "2063: Aligned TAI does not end time of perils\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 7 ##\n",
      "############\n",
      "--- /!\\ TAI CREATED in 2046\n",
      "2047: ...XRISK from AI takeover (singleton) :(\n",
      "...Singleton is catastrophic\n",
      "...but the TAI singleton is still good because the TAI's values are still morally good.\n",
      "label for this FUTURE => xrisk_full_unaligned_but_morally_good_tai_singleton\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 8 ##\n",
      "############\n",
      "2025: A country other than Russia/China/NK uses a nuke first strike (outside of great power war)!\n",
      "2026: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 7 years (total delay 7 years)\n",
      "2029: War ends :)\n",
      "--- /!\\ TAI CREATED in 2071\n",
      "2072: ...coordinated to not deploy TAI\n",
      "2073: ...Achieved aligned TAI (aligned by default)\n",
      "2073: Aligned TAI does not end time of perils\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 9 ##\n",
      "############\n",
      "--- /!\\ TAI CREATED in 2034\n",
      "2035: ...XRISK from AI takeover (singleton) :(\n",
      "...Singleton is catastrophic\n",
      "...but the TAI singleton is still good because the TAI's values are still morally good.\n",
      "label for this FUTURE => xrisk_full_unaligned_but_morally_good_tai_singleton\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 10 ##\n",
      "############\n",
      "2033: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 10 years (total delay 10 years)\n",
      "2043: War ends :)\n",
      "2058: North Korea uses a nuke first strike!\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 11 ##\n",
      "############\n",
      "2029: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 12 years (total delay 12 years)\n",
      "2049: War ends :)\n",
      "--- /!\\ TAI CREATED in 2053\n",
      "2054: ...Achieved aligned TAI (aligned by default)\n",
      "2054: Aligned TAI does not end time of perils\n",
      "2113: WAR!!! (US vs. Russia)\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 12 ##\n",
      "############\n",
      "2024: North Korea uses a nuke first strike!\n",
      "--- /!\\ TAI CREATED in 2028\n",
      "2029: ...XRISK from AI takeover (singleton) :(\n",
      "...Singleton is catastrophic\n",
      "label for this FUTURE => xrisk_full_unaligned_tai_singleton\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 13 ##\n",
      "############\n",
      "2026: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 14 years (total delay 14 years)\n",
      "2035: War ends :)\n",
      "--- /!\\ TAI CREATED in 2081\n",
      "2082: ...Achieved aligned TAI (aligned by default)\n",
      "2082: Aligned TAI does not end time of perils\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 14 ##\n",
      "############\n",
      "2025: WAR!!! (US vs. Russia)\n",
      "--- /!\\ TAI CREATED in 2032\n",
      "2033: ...XRISK from AI takeover (singleton) :(\n",
      "label for this FUTURE => xrisk_full_unaligned_tai_singleton\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 15 ##\n",
      "############\n",
      "--- /!\\ TAI CREATED in 2035\n",
      "2036: ...Achieved aligned TAI (aligned by default)\n",
      "2036: Aligned TAI does not end time of perils\n",
      "2112: WAR!!! (US vs. Russia)\n",
      "2120: ...catastrophe from pathogen (war)\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 16 ##\n",
      "############\n",
      "2029: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 10 years (total delay 10 years)\n",
      "2033: War ends :)\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 17 ##\n",
      "############\n",
      "2028: North Korea uses a nuke first strike!\n",
      "--- /!\\ TAI CREATED in 2082\n",
      "2083: ...coordinated to not deploy TAI\n",
      "2084: ...XRISK from Intentional misuse of TAI (singleton) :(\n",
      "label for this FUTURE => xrisk_tai_misuse\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 18 ##\n",
      "############\n",
      "2074: WAR!!! (Other)\n",
      "2086: War ends :)\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 19 ##\n",
      "############\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 20 ##\n",
      "############\n",
      "2081: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 7 years (total delay 7 years)\n",
      "2089: War ends :)\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 21 ##\n",
      "############\n",
      "2085: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 9 years (total delay 9 years)\n",
      "2098: War ends :)\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 22 ##\n",
      "############\n",
      "--- /!\\ TAI CREATED in 2050\n",
      "2051: ...Achieved aligned TAI (aligned via work, first attempt)\n",
      "2051: Aligned TAI does not end time of perils\n",
      "2082: WAR!!! (US vs. China)\n",
      "2086: War ends :)\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 23 ##\n",
      "############\n",
      "2045: WAR!!! (US vs. Russia)\n",
      "2046: War ends :)\n",
      "--- /!\\ TAI CREATED in 2058\n",
      "2059: ...Achieved aligned TAI (aligned by default)\n",
      "2059: Aligned TAI does not end time of perils\n",
      "...We survive to >2123 with aligned TAI but still some perils\n",
      "label for this FUTURE => aligned_tai_does_not_end_time_of_perils\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 24 ##\n",
      "############\n",
      "2028: WAR!!! (US vs. China)\n",
      "...US-China war delays TAI by 5 years (total delay 5 years)\n",
      "2040: War ends :)\n",
      "2105: ...catastrophe from natural pathogen\n",
      "...catastrophe delays TAI by 12 years (total delay 17 years)\n",
      "...We survive to >2123 with a boring future (no TAI)\n",
      "label for this FUTURE => boring\n",
      "-\n",
      "-\n",
      "############\n",
      "## RUN 25 ##\n",
      "############\n",
      "--- /!\\ TAI CREATED in 2048\n",
      "2049: ...XRISK from Intentional misuse of TAI (singleton) :(\n",
      "...Singleton is catastrophic\n",
      "label for this FUTURE => xrisk_tai_misuse\n",
      "-\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "exec(open('modules/define_event.py').read())\n",
    "\n",
    "for i in range(5):\n",
    "    print('############')\n",
    "    print('## RUN {} ##'.format(i + 1))\n",
    "    print('############')\n",
    "    define_event(VARS, verbosity=2)\n",
    "\n",
    "for i in range(5, 25):\n",
    "    print('############')\n",
    "    print('## RUN {} ##'.format(i + 1))\n",
    "    print('############')\n",
    "    define_event(VARS, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading cache...\n",
      "Generating Bayes net with 12 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100044it [02:28, 674.09it/s]                                                                                                                                                                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling data...\n",
      "Waiting for other cores...\n",
      "Collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                         | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test-core-3.sqcache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/dev/lib/python3.11/site-packages/squigglepy/bayes.py:243\u001b[0m, in \u001b[0;36mbayesnet\u001b[0;34m(event_fn, n, find, conditional_on, reduce_fn, raw, memcache, memcache_load, memcache_save, reload_cache, dump_cache_file, load_cache_file, cache_file_primary, verbose, cores)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cores):\n\u001b[1;32m    242\u001b[0m     _tick_tqdm(pbar, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest-core-\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.sqcache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[1;32m    244\u001b[0m         events \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(infile\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    245\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-core-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.sqcache\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(c))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test-core-3.sqcache'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "define_event_lambda = lambda: define_event(VARS, verbosity=0)\n",
    "collectors = bayes.bayesnet(define_event_lambda,\n",
    "                            load_cache_file='caches/future_assessment_model_cache',\n",
    "                            dump_cache_file='caches/future_assessment_model_cache',\n",
    "                            reload_cache=True,\n",
    "                            raw=True,\n",
    "                            verbose=True,\n",
    "                            cores=max(1, os.cpu_count() - 2),\n",
    "                            n=VARS['RUNS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4E. When TAI?\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████████████████████████████████████████████████▋                                                                                                                                           | 4/12 [01:36<03:12, 24.09s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "State xrisk_{'catastrophe': 'nukes_accident', 'year': 2037} not in `FUTURES`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4E. When TAI?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m yrs \u001b[38;5;241m=\u001b[39m \u001b[43mbayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbayesnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefine_event_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mfind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtai_year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVARS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRUNS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m yrs \u001b[38;5;241m=\u001b[39m [VARS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAX_YEAR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m y \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m yrs]\n\u001b[1;32m      9\u001b[0m print_tai_arrival_stats(yrs, VARS)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/dev/lib/python3.11/site-packages/squigglepy/bayes.py:202\u001b[0m, in \u001b[0;36mbayesnet\u001b[0;34m(event_fn, n, find, conditional_on, reduce_fn, raw, memcache, memcache_load, memcache_save, reload_cache, dump_cache_file, load_cache_file, cache_file_primary, verbose, cores)\u001b[0m\n\u001b[1;32m    200\u001b[0m     r_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(n)\n\u001b[1;32m    201\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m _init_tqdm(verbose\u001b[38;5;241m=\u001b[39mverbose, total\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m--> 202\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mrun_event_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr_\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    203\u001b[0m     _flush_tqdm(pbar)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/dev/lib/python3.11/site-packages/squigglepy/bayes.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    200\u001b[0m     r_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(n)\n\u001b[1;32m    201\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m _init_tqdm(verbose\u001b[38;5;241m=\u001b[39mverbose, total\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m--> 202\u001b[0m     events \u001b[38;5;241m=\u001b[39m [\u001b[43mrun_event_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m r_]\n\u001b[1;32m    203\u001b[0m     _flush_tqdm(pbar)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/dev/lib/python3.11/site-packages/squigglepy/bayes.py:195\u001b[0m, in \u001b[0;36mbayesnet.<locals>.run_event_fn\u001b[0;34m(pbar, total_cores)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_event_fn\u001b[39m(pbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, total_cores\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    194\u001b[0m     _tick_tqdm(pbar, total_cores)\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevent_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<string>:106\u001b[0m, in \u001b[0;36mdefine_event\u001b[0;34m(variables, verbosity)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: State xrisk_{'catastrophe': 'nukes_accident', 'year': 2037} not in `FUTURES`"
     ]
    }
   ],
   "source": [
    "print('4E. When TAI?')\n",
    "print('-')\n",
    "\n",
    "yrs = bayes.bayesnet(define_event_lambda,\n",
    "                     find=lambda e: e['tai_year'],\n",
    "                     raw=True,\n",
    "                     n=VARS['RUNS'])\n",
    "yrs = [VARS['MAX_YEAR'] + 1 if y is None else y for y in yrs]\n",
    "print_tai_arrival_stats(yrs, VARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if years[-1] > 2200:\n",
    "    years = list(range(VARS['CURRENT_YEAR'], 2200))\n",
    "\n",
    "alignment_p = np.array([p_alignment_solved(year=y - VARS['CURRENT_YEAR'], first_attempt=True) for y in years])\n",
    "alignment_p2 = np.array([p_alignment_solved(year=y - VARS['CURRENT_YEAR'], first_attempt=False) for y in years])\n",
    "plt.plot(years, alignment_p, label='first attempt')\n",
    "plt.plot(years, alignment_p2, label='2+ attempt')\n",
    "plt.legend()\n",
    "plt.ylabel('chance of solving alignment if TAI in year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in list(years[:17]) + list(years[17::10]):\n",
    "    str_ = 'Year: {} - chance of solving TAI alignment {}% (2nd attempt {}%)'\n",
    "    print(str_.format(y,\n",
    "                      round(alignment_p[y - VARS['CURRENT_YEAR']] * 100, 0),\n",
    "                      round(alignment_p2[y - VARS['CURRENT_YEAR']] * 100, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordination_p = np.array([p_alignment_deployment_safety_and_coordination(year=y - VARS['CURRENT_YEAR'], war=False, variables=VARS, first_attempt=True) for y in years])\n",
    "coordination_p2 = np.array([p_alignment_deployment_safety_and_coordination(year=y - VARS['CURRENT_YEAR'], war=False, variables=VARS, first_attempt=False) for y in years])\n",
    "coordination_p_war = np.array([p_alignment_deployment_safety_and_coordination(year=y - VARS['CURRENT_YEAR'], war=True, variables=VARS, first_attempt=True) for y in years])\n",
    "coordination_p2_war = np.array([p_alignment_deployment_safety_and_coordination(year=y - VARS['CURRENT_YEAR'], war=True, variables=VARS, first_attempt=False) for y in years])\n",
    "plt.plot(years, coordination_p, label='no war, first attempt')\n",
    "plt.plot(years, coordination_p2, label='no war, 2+ attempt')\n",
    "plt.plot(years, coordination_p_war, label='war, first attempt')\n",
    "plt.plot(years, coordination_p2_war, label='war, 2+ attempt')\n",
    "plt.legend()\n",
    "plt.ylabel('chance of coordinating deployment if TAI in year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in list(years[:17]) + list(years[17::10]):\n",
    "    str_ = 'Year: {} - chance of coordinating deployment no war -- {}% (2nd attempt {}%); war -- {}% (2nd attempt {}%)'\n",
    "    print(str_.format(y,\n",
    "                      round(coordination_p[y - VARS['CURRENT_YEAR']] * 100, 0),\n",
    "                      round(coordination_p2[y - VARS['CURRENT_YEAR']] * 100, 0),\n",
    "                      round(coordination_p_war[y - VARS['CURRENT_YEAR']] * 100, 0),\n",
    "                      round(coordination_p2_war[y - VARS['CURRENT_YEAR']] * 100, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AI X-Risk BY EOY year')\n",
    "\n",
    "def find(y_c, category):\n",
    "    return bayes.bayesnet(define_event_lambda,\n",
    "                          find=lambda e: e['category'] == category and e['final_year'] <= y_c,\n",
    "                          n=VARS['RUNS'])\n",
    "\n",
    "target_years = [2024, 2025, 2026, 2027, 2028, 2030, 2035, 2040, 2050, 2060, 2070, 2100]\n",
    "target_years = [y for y in target_years if y <= VARS['MAX_YEAR']]\n",
    "for y_c in target_years:\n",
    "    extinction = find(y_c, 'xrisk_full_unaligned_tai_extinction')\n",
    "    singleton = find(y_c, 'xrisk_full_unaligned_tai_singleton')\n",
    "    subtle_misalignment = find(y_c, 'xrisk_subtly_unaligned_tai')\n",
    "    misuse = find(y_c, 'xrisk_tai_misuse') + find(y_c, 'xrisk_tai_misuse_extinction')\n",
    "    out = '{} - {}% (Extinction: {}%, Bad TAI singleton: {}%, Subtly misaligned singleton: {}%, Misuse singleton: {}%)'\n",
    "    print(out.format(y_c,\n",
    "                     round((extinction + singleton + subtle_misalignment + misuse) * 100, 1),\n",
    "                     round(extinction * 100, 1),\n",
    "                     round(singleton * 100, 1),\n",
    "                     round(subtle_misalignment * 100, 1),\n",
    "                     round(misuse * 100, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Chance of successfully aligning TAI by EOY year')\n",
    "for y_c in target_years:\n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: e['tai_type'] == 'aligned_agent' and e['tai_year'] <= y_c,\n",
    "                       n=VARS['RUNS'])\n",
    "    r1 = bayes.bayesnet(define_event_lambda,\n",
    "                        find=lambda e: e['category'] == 'aligned_tai_ends_time_of_perils' and e['final_year'] <= y_c,\n",
    "                        n=VARS['RUNS'])\n",
    "    print('{} - {}% ({}% ends time of perils, {}% does not)'.format(y_c,\n",
    "                                                                    round(r * 100, 2),\n",
    "                                                                    round(r1 * 100, 2),\n",
    "                                                                    round((r - r1) * 100, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cumulative Total X-Risk (including non-extinction x-risks and \"ok but not great\" x-risks)')\n",
    "\n",
    "def find_xrisk(y_c, category):\n",
    "    return bayes.bayesnet(define_event_lambda,\n",
    "                          find=lambda e: 'xrisk' in e['category'] and category in e['category'] and e['final_year'] <= y_c,\n",
    "                          n=VARS['RUNS'])\n",
    "\n",
    "for y_c in target_years:\n",
    "    ai = find_xrisk(y_c, 'tai')\n",
    "    nukes = find_xrisk(y_c, 'nukes')\n",
    "    unknown = find_xrisk(y_c, 'unknown')\n",
    "    nano = find_xrisk(y_c, 'nanotech')\n",
    "    natural = find_xrisk(y_c, 'supervolcano')\n",
    "    bio = find_xrisk(y_c, 'bio')\n",
    "\n",
    "    out = '{} - {}% (AI: {}%, Nukes: {}%, Bio: {}%, Nano: {}%, Natural: {}%, Unknown unknown: {}%)'\n",
    "    print(out.format(y_c,\n",
    "                     round((ai + nukes + bio + nano + unknown) * 100, 2),\n",
    "                     round(ai * 100, 2),\n",
    "                     round(nukes * 100, 3),\n",
    "                     round(bio * 100, 3),\n",
    "                     round(nano * 100, 3),\n",
    "                     round(natural * 100, 3),\n",
    "                     round(unknown * 100, 3)))\n",
    "\n",
    "print('-')\n",
    "print('AI risk is {}% of total risk'.format(round(ai / (ai + nukes + bio + nano + unknown) * 100, 2)))\n",
    "print('AI risk is {}% of known risk'.format(round(ai / (ai + nukes + bio + nano) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cumulative Total Extinction Risk')\n",
    "\n",
    "for y_c in target_years:\n",
    "    ai = find(y_c, 'xrisk_full_unaligned_tai_extinction') + find(y_c, 'xrisk_tai_misuse_extinction')\n",
    "    nukes_war = find(y_c, 'xrisk_nukes_war')\n",
    "    nukes_accident = find(y_c, 'xrisk_nukes_accident')\n",
    "    unknown = find(y_c, 'xrisk_unknown_unknown')\n",
    "    bio_war = find(y_c, 'xrisk_bio_war')\n",
    "    bio_accident = find(y_c, 'xrisk_bio_accident')\n",
    "    bio_nonstate = find(y_c, 'xrisk_bio_nonstate')\n",
    "    nanotech = find(y_c, 'xrisk_nanotech')\n",
    "    supervolcano = find(y_c, 'xrisk_supervolcano')\n",
    "    \n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: e['category'] in extinctions and e['final_year'] <= y_c,\n",
    "                       n=VARS['RUNS'])\n",
    "    \n",
    "    out = '{} - {}% (AI: {}%, Nukes: {}% (War: {}% Accident: {}%), Bio: {}% (War: {}%, Accident: {}%, Nonstate: {}%), Nano: {}%, Natural: {}%, Other: {}%)'\n",
    "    print(out.format(y_c,\n",
    "                     round(r * 100, 2),\n",
    "                     round(ai * 100, 2),\n",
    "                     round((nukes_war + nukes_accident) * 100, 3),\n",
    "                     round(nukes_war * 100, 3),\n",
    "                     round(nukes_accident * 100, 3),\n",
    "                     round((bio_war + bio_accident + bio_nonstate) * 100, 3),\n",
    "                     round(bio_war * 100, 3),\n",
    "                     round(bio_accident * 100, 3),\n",
    "                     round(bio_nonstate * 100, 3),\n",
    "                     round(nanotech * 100, 3),\n",
    "                     round(supervolcano * 100, 3),\n",
    "                     round(unknown * 100, 3)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(bayes.bayesnet(define_event_lambda, find=lambda e: e['category'], raw=True, n=VARS['RUNS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.metaculus.com/questions/1493/global-population-decline-10-by-2100/\n",
    "print('Cumulative Total Catastrophe Risk (defined as 10%+ death)')\n",
    "\n",
    "for y_c in target_years:\n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: (len(e['catastrophe']) > 0 and e['catastrophe'][0]['year'] <= y_c) or (e['category'] in extinctions and e['final_year'] <= y_c),\n",
    "                       n=VARS['RUNS'])\n",
    "    print('{} - {}%'.format(y_c, round(r * 100, 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cumulative Total *Actively Bad* Future X-Risk (including non-extinction risks but excluding subtle AI misalignment)')\n",
    "\n",
    "for y_c in target_years:\n",
    "    ai = find_xrisk(y_c, 'tai')\n",
    "    ai_subtle_misalignment = find_xrisk(y_c, 'xrisk_subtly_unaligned_tai')\n",
    "    nukes = find_xrisk(y_c, 'nukes')\n",
    "    unknown = find_xrisk(y_c, 'unknown')\n",
    "    nano = find_xrisk(y_c, 'nanotech')\n",
    "    natural = find_xrisk(y_c, 'supervolcano')\n",
    "    bio = find_xrisk(y_c, 'bio')\n",
    "\n",
    "    out = '{} - {}% (AI: {}%, Nukes: {}%, Bio: {}%, Nano: {}%, Natural: {}%, Other: {}%)'\n",
    "    print(out.format(y_c,\n",
    "                     round((ai - ai_subtle_misalignment + nukes + bio + nano + unknown) * 100, 2),\n",
    "                     round((ai - ai_subtle_misalignment) * 100, 2),\n",
    "                     round(nukes * 100, 3),\n",
    "                     round(bio * 100, 3),\n",
    "                     round(nano * 100, 3),\n",
    "                     round(natural * 100, 3),\n",
    "                     round(unknown * 100, 3)))\n",
    "\n",
    "print('-')\n",
    "print('AI risk is {}% of total risk'.format(round((ai - ai_subtle_misalignment) / ((ai - ai_subtle_misalignment) + nukes + bio + nano + unknown) * 100, 2)))\n",
    "print('AI risk is {}% of known risk'.format(round((ai - ai_subtle_misalignment) / ((ai - ai_subtle_misalignment) + nukes + bio + nano) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('File last ran: {}'.format(dt.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_states(collectors, y_c):\n",
    "    states = [c['category'] if (not isinstance(c['final_year'], str) and c['final_year'] <= y_c) else 'boring' for c in collectors]\n",
    "    c = Counter(states)\n",
    "    c = dict([(k, round(v / VARS['RUNS'] * 100, 3)) for k, v in c.items()])\n",
    "    for k in c.keys():\n",
    "        if k not in FUTURES:\n",
    "            raise ValueError('Future {} not in `FUTURES`'.format(k))\n",
    "    for state in FUTURES:\n",
    "        if not c.get(state):\n",
    "            c[state] = 0.0\n",
    "    return sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "print('Detail on World State At Year')\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c)) \n",
    "    pprint(print_states(collectors, y_c))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on Catastrophe States At Year')\n",
    "\n",
    "def print_catastrophe(collectors, y_c):\n",
    "    catastrophes = [[('', 0)] if len(cs['catastrophe']) == 0 else [(c['catastrophe'], c['year']) for c in cs['catastrophe']] for cs in collectors]\n",
    "    catastrophes = [c  if c[0][1] <= y_c else '' for c in catastrophes]\n",
    "    c = Counter([' '.join(sorted([c[0] for c in cs])) for cs in catastrophes])\n",
    "    c = dict([(k, round(v / VARS['RUNS'] * 100, 2)) for k, v in c.items()])\n",
    "    c = sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "    c = [c_ for c_ in c if c_[1] >= 0.1]\n",
    "    return c\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    pprint(print_catastrophe(collectors, y_c))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on *First* Catastrophe State At Year')\n",
    "\n",
    "def print_catastrophe_first(collectors, y_c):\n",
    "    catastrophes = [[('', 0)] if len(cs['catastrophe']) == 0 else [(c['catastrophe'], c['year']) for c in cs['catastrophe']] for cs in collectors]\n",
    "    c = Counter([c[0][0] if c[0][1] <= y_c else '' for c in catastrophes])\n",
    "    c = dict([(k, round(v / VARS['RUNS'] * 100, 2)) for k, v in c.items()])\n",
    "    c = sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "    c = [c_ for c_ in c if c_[1] >= 0.1]\n",
    "    return c\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    pprint(print_catastrophe_first(collectors, y_c))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on # of Catastrophes At Year')\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## # of catastrophes as of {} ##'.format(y_c))  \n",
    "    pprint(sq.get_percentiles([len([c for c in cs['catastrophe'] if c['year'] <= y_c]) for cs in collectors]))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on Double Catastrophe X-Risks')\n",
    "\n",
    "def print_double_catastrophes(catastrophes):\n",
    "    c = Counter(['' if c is None else c for c in catastrophes])\n",
    "    c = dict([(k, round(v / VARS['RUNS'] * 100, 3)) for k, v in c.items()])\n",
    "    c = sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
    "    # c = [c_ for c_ in c if c_[1] >= 0.1]\n",
    "    return c\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    pprint(print_double_catastrophes(['' if (not isinstance(c['final_year'], str) and c['final_year'] > y_c) else c['double_catastrophe_xrisk'] for c in collectors]))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_found = False\n",
    "i = 0\n",
    "y_range = list(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))\n",
    "trigger_found = False\n",
    "while not trigger_found and i < len(y_range):\n",
    "    print('.', end='')\n",
    "    y_c = y_range[i]\n",
    "    ai = find_xrisk(y_c, 'tai')\n",
    "    nukes = find_xrisk(y_c, 'nukes')\n",
    "    unknown = find_xrisk(y_c, 'unknown')\n",
    "    nano = find_xrisk(y_c, 'nanotech')\n",
    "    natural = find_xrisk(y_c, 'supervolcano')\n",
    "    bio = find_xrisk(y_c, 'bio')\n",
    "    xrisk = ai + nukes + bio + nano + unknown\n",
    "    if xrisk > 0.05:\n",
    "        trigger_found = True\n",
    "        y_c -= (xrisk - 0.05)/0.05\n",
    "    i += 1\n",
    "\n",
    "print('')\n",
    "if not trigger_found:\n",
    "    y_c = '>{}'.format(VARS['MAX_YEAR'])\n",
    "else:\n",
    "    y_c = dt.strftime(dt.now() + timedelta(days=(y_c - 2023) * 365.24), '%Y %b %d')\n",
    "    \n",
    "print('5% X-Risk By {}'.format(y_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total X-Risk IN THAT SPECIFIC YEAR (non-cumulative) (including non-extinction x-risks)')\n",
    "\n",
    "print('1/6: AI')\n",
    "ai = np.diff(np.array([find_xrisk(y, 'tai') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('2/6: Nukes')\n",
    "nukes = np.diff(np.array([find_xrisk(y, 'nukes') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('3/6: Unknown')\n",
    "unknown = np.diff(np.array([find_xrisk(y, 'unknown') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('4/6: Nanotech')\n",
    "nano = np.diff(np.array([find_xrisk(y, 'nanotech') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('5/6: Natural risk')\n",
    "natural = np.diff(np.array([find_xrisk(y, 'supervolcano') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "print('6/6: Biorisk')\n",
    "bio = np.diff(np.array([find_xrisk(y, 'bio') for y in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR']))]))\n",
    "\n",
    "xrisk_df = pd.DataFrame({'year': range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR'] - 1),\n",
    "                         'ai': ai,\n",
    "                         'nukes': nukes,\n",
    "                         'unknown': unknown,\n",
    "                         'nano': nano,\n",
    "                         'natural': natural,\n",
    "                         'bio': bio})\n",
    "xrisk_df['total'] = xrisk_df['ai'] + xrisk_df['nukes'] + xrisk_df['unknown'] + xrisk_df['nano'] + xrisk_df['natural'] + xrisk_df['bio']\n",
    "xrisk_df.to_csv('caches/xrisk_df.csv', index=False)\n",
    "xrisk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for y_c in tqdm(range(VARS['CURRENT_YEAR'], VARS['MAX_YEAR'] - 1)):\n",
    "    state = dict([s for s in print_states(collectors, y_c) if 'morally_good_actually' not in s[0]])\n",
    "    state['year'] = y_c\n",
    "    all_states.append(state)\n",
    "\n",
    "world_states_df = pd.DataFrame(all_states)\n",
    "world_states_df.to_csv('caches/world_states_df.csv', index=False)\n",
    "world_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total X-Risk OR catastrophe by EOY year')\n",
    "\n",
    "for y_c in target_years:\n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: ('xrisk' in e['category'] and e['final_year'] <= y_c) or (len(e['catastrophe']) > 0 and e['catastrophe'][0]['year'] <= y_c),\n",
    "                       n=VARS['RUNS'])\n",
    "    print('{} - {}%'.format(y_c, round(r * 100, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total X-Risk AND catastrophe by EOY year')\n",
    "\n",
    "for y_c in target_years:\n",
    "    r = bayes.bayesnet(define_event_lambda,\n",
    "                       find=lambda e: ('xrisk' in e['category'] and e['final_year'] <= y_c) and (len(e['catastrophe']) > 0 and e['catastrophe'][0]['year'] <= y_c),\n",
    "                       n=VARS['RUNS'])\n",
    "    print('{} - {}%'.format(y_c, round(r * 100, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://forum.effectivealtruism.org/posts/nYgw4FNpHf9bmJGEi/forecasting-thread-how-does-ai-risk-level-vary-based-on\n",
    "\n",
    "def generate_conditional(y_low, y_high):\n",
    "    def fn(e):\n",
    "        if e['tai_year'] is None:\n",
    "            return False\n",
    "        elif e['tai_year'] < y_low:\n",
    "            return False\n",
    "        elif e['tai_year'] > y_high:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    return fn\n",
    "    \n",
    "\n",
    "def find(y_low, y_high, category):\n",
    "    try:\n",
    "        return bayes.bayesnet(define_event_lambda,\n",
    "                              find=lambda e: e['category'] == category and e['final_year'] <= (VARS['MAX_YEAR'] - 1 if y_high >= VARS['MAX_YEAR'] else y_high),\n",
    "                              conditional_on=generate_conditional(y_low, y_high),\n",
    "                              n=VARS['RUNS'])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "for y_c in [[2022, 2029], [2029, 2039], [2039, 2059], [2059, VARS['MAX_YEAR'] - 1], [2022, 2070]]:\n",
    "    print('AI X-Risk conditional on TAI beween beginning of {} and end of {}'.format(y_c[0] + 1, y_c[1]))\n",
    "    extinction = find(y_c[0], y_c[1], 'xrisk_full_unaligned_tai_extinction')\n",
    "    singleton = find(y_c[0], y_c[1], 'xrisk_full_unaligned_tai_singleton')\n",
    "    subtle_misalignment = find(y_c[0], y_c[1], 'xrisk_subtly_unaligned_tai')\n",
    "    misuse = find(y_c[0], y_c[1], 'xrisk_tai_misuse')\n",
    "    out = '{}% (Extinction: {}%, Bad TAI singleton: {}%, Subtly misaligned singleton: {}%, Misuse singleton: {}%)'\n",
    "    print(out.format(round((extinction + singleton + subtle_misalignment + misuse) * 100, 1),\n",
    "                     round(extinction * 100, 1),\n",
    "                     round(singleton * 100, 1),\n",
    "                     round(subtle_misalignment * 100, 1),\n",
    "                     round(misuse * 100, 1)))\n",
    "    print('-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Have we seen the following wars by EOY year?')\n",
    "\n",
    "def print_wars(wars, y_c):\n",
    "    bs = [[w['belligerents'] for w in ws if w != [] and w['start_year'] <= y_c] for ws in wars]\n",
    "    bs = Counter([' '.join(sorted(b)) for b in bs])    \n",
    "    bs = dict([(k, round(v / VARS['RUNS'] * 100, 3)) for k, v in bs.items()])\n",
    "    bs = sorted(bs.items(), key=lambda x: x[1], reverse=True)\n",
    "    for war in ['US/China', 'US/Russia', 'Other']:\n",
    "        print('{}: {}%'.format(war, round(sum([b[1] if war in b[0] else 0 for b in bs]), 1)))\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    print_wars([c['wars'] for c in collectors], y_c)\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Offensive nuclear weapon use (1+ fatality) by EOY year?')\n",
    "\n",
    "def p_nuke_used_by(y_c):\n",
    "    return bayes.bayesnet(define_event_lambda,\n",
    "                          find=lambda e: len(e['nuclear_weapon_used']) > 0 and e['nuclear_weapon_used'][0] <= y_c,\n",
    "                          n=VARS['RUNS'])\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('{}: {}%'.format(y_c, round(p_nuke_used_by(y_c) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on war states')\n",
    "\n",
    "def print_wars(wars, y_c):\n",
    "    bs = [[w['belligerents'] for w in ws if w != [] and w['start_year'] <= y_c] for ws in wars]\n",
    "    bs = Counter([' '.join(sorted(b)) for b in bs])    \n",
    "    bs = dict([(k, round(v / VARS['RUNS'] * 100, 3)) for k, v in bs.items()])\n",
    "    return sorted(bs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## {} ##'.format(y_c))  \n",
    "    pprint(print_wars([c['wars'] for c in collectors], y_c))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on # of Wars At Year')\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## # of wars as of {} ##'.format(y_c))  \n",
    "    pprint(sq.get_percentiles([len([c for c in cs['wars'] if c['start_year'] <= y_c]) for cs in collectors]))\n",
    "    print('-')\n",
    "    print('-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Detail on War Length States At Year')\n",
    "\n",
    "def print_wars(wars, y_c):\n",
    "    bs = [[(w['end_year'] - w['start_year'] if w['end_year'] < y_c else y - w['start_year']) if w != [] and w['start_year'] <= y_c else 0 for w in ws] for ws in wars]\n",
    "    bs = [round(sum(b) / (y - VARS['CURRENT_YEAR']) * 100, 1) for b in bs]\n",
    "    return bs\n",
    "\n",
    "for y_c in target_years:\n",
    "    print('## Percent of time in war as of {} ##'.format(y_c))  \n",
    "    pprint(sq.get_percentiles(print_wars([c['wars'] for c in collectors], y_c)))\n",
    "    print('-')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('File last ran: {}'.format(dt.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
