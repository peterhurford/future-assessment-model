// Based on Ajeya Cotra's 2020 "When compute required to train a transformative model may be attainable".
// - https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines
// 
// Adapted by: Jesse Hoogland

// Best guess 
// - https://docs.google.com/spreadsheets/d/1TjNQyVHvHlC-sZbcA7CRKcCp0NxV6MkkqBvL408xrJw/edit#gid=505210495

start_year = 2022
end_year = 2100

minx(n, max) = { if n > max then max else n }
maxx(n, min) = { if n < min then min else n }
bound(x, min, max) = maxx(minx(x, max), min)

sminx(n, max) = SampleSet.max(n, max)
smaxx(n, min) = SampleSet.min(n, min)
sbound(x, min, max) = smaxx(sminx(x, max), min)

bayes_update_against_low_end_flop(anchor) = anchor |> truncateLeft(26)

anchor(brain, efficiency, transformative_vs_human, horizon_length,
       scaling_exponent, params, ref_params, ref_params_samples) = {
           
    anchor = brain + efficiency + transformative_vs_human +
             horizon_length + ref_params_samples -
             scaling_exponent * ref_params +
             scaling_exponent * params
             
    anchor |> bayes_update_against_low_end_flop
}

mixed_PDF = {
    brain = 11 to 19.5
    efficiency = 1 to 3
    horizon = mixture(0 to .1,
                      1 to 3,
                      3 to 7,
                      7 to 10,
                      [.4, .25, .3, .05])
    transformative_vs_human = -2 to 2
    scaling_exponent = 0.5 to 1.5
    flops_per_param_per_sec = 1 to 2
    params = brain + efficiency - flops_per_param_per_sec
    ref_params = 11.2
    ref_params_samples = 12
    
    anchor(brain, efficiency, transformative_vs_human, horizon,
           scaling_exponent, params, ref_params, ref_params_samples)
}

flop_per_$(y) = {
    curr_FLOP_per_$ = 1e18
    compute_$_halving_time = 3.2
    max_FLOP_per_$ = 1e26
    t = y - start_year
    curr_FLOP_per_$ * (exp((log(2) / compute_$_halving_time) * t)) / (1 + curr_FLOP_per_$ / max_FLOP_per_$ * exp(log(2) / compute_$_halving_time * t))  
}

frontier_country_GDP(y) = {
    // Our reference measurement comes from 2020, not 2025.
    t = y - 2020
    frontier_country_GDP_2020 = 2.13e13
    growth_rate = .025
    frontier_country_GDP_2020 * (1 + growth_rate)^(t)
}

spend(y) = {
    initial_pay = 2e7 // in 2020 USD
    spend_doubling_time = 3
    gdp = frontier_country_GDP(y)
    max_gdp_frac = 0.0007
    t = y - start_year
    x = (log(2) / spend_doubling_time) * t
    10 ^ (log10(initial_pay) + log10(exp(x)) - log10(1 + initial_pay / (gdp * max_gdp_frac) * exp(x)))
}

flop(y) = {
    flop_per_$(y) * spend(y)
}

logflop_at_y(y) = log10(flop(y))

algo_progress_reduce_at_y_for_f(y, f) = {
    max_algo_reduction = 10
    algo_halving_time = 2
    t = y - start_year
    r = (exp((log(2) / algo_halving_time) * t)) / (1 + 1 / max_algo_reduction * exp(log(2) / algo_halving_time * t))
    f - r
}

flop_to_make_tai_at_y(y) = {
    SampleSet.map(SampleSet.fromDist(mixed_PDF),
                  { |f| algo_progress_reduce_at_y_for_f(y, f) })  |> truncateLeft(26)
}

add_nonscaling_delay(y) = {
    t = y - start_year
    round((1 / t) * 4)
}

mixed(y) = {
    y_ = y //- add_nonscaling_delay(y)
    cdf(flop_to_make_tai_at_y(y), logflop_at_y(y_))
}

{
    '2025': mixed(2025),
    '2030': mixed(2030),
    '2032': mixed(2032),
    '2036': mixed(2036),
    '2040': mixed(2040),
    '2050': mixed(2050),
    '2055': mixed(2055),
    '2060': mixed(2060),
    '2070': mixed(2070),
    '2080': mixed(2080),
    '2090': mixed(2090),
    '2100': mixed(2100)
}

